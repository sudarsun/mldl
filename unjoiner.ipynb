{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-24 19:49:54.140748: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-24 19:49:54.310348: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-24 19:49:54.903325: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:\n",
      "2022-11-24 19:49:54.903417: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:\n",
      "2022-11-24 19:49:54.903426: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, GRU, Dropout, Conv1D\n",
    "from keras.layers import Bidirectional, concatenate, SpatialDropout1D, GlobalMaxPooling1D\n",
    "from keras.layers import Input\n",
    "import pickle\n",
    "from keras.utils import to_categorical, pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "from keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"CharLSTM+ConjoinedWords\"\n",
    "SPECIALITY = \"emed\"\n",
    "DATA_PATH = \"data/EM_ED_240222/\"\n",
    "DATA_VERSION = 'EM_ED_240222'\n",
    "file_path = 'best_model_'+MODEL+'_'+SPECIALITY+'_'+DATA_VERSION+'_'+str(datetime.datetime.now())\n",
    "\n",
    "MIN_WORD_FREQUENCY = 5\n",
    "max_len = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 100\n",
    "max_len_char = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences = open(DATA_PATH+SPECIALITY+'_train.tokens',\"r\").readlines()[:10000]\n",
    "len(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_sentences = open(DATA_PATH+SPECIALITY+'_val.tokens',\"r\").readlines()[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def process_sentences(sentences, n = 4):\n",
    "    out_sentences = []\n",
    "    out_tags = []\n",
    "    for sentence in sentences:\n",
    "        x = []\n",
    "        y = []\n",
    "        stripped_sentence = sentence.strip().lower().replace(\" \", \"\")\n",
    "        for shingle in [stripped_sentence[k:k+n] for k in range(len(stripped_sentence)-n+1)]:\n",
    "            x.append(shingle)\n",
    "            if shingle in sentence.lower():\n",
    "                y.append(0)\n",
    "            else:\n",
    "                y.append(1)\n",
    "        out_sentences.append(x)\n",
    "        out_tags.append(y)\n",
    "    return out_sentences, out_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "\n",
    "# def process_sentences(sentences, n = 4):\n",
    "#     out_sentences = []\n",
    "#     out_tags = []\n",
    "#     for i in range(len(sentences)):\n",
    "#         try:\n",
    "#             sentence = sentences[i].strip().lower()\n",
    "#             x = []\n",
    "#             tags = []\n",
    "#             head = 0\n",
    "#             while head < len(sentence)-n+1:\n",
    "#                 curr_shingle = ''\n",
    "#                 pointer = head\n",
    "#                 tag = 0\n",
    "#                 while len(curr_shingle) < n:\n",
    "#                     if pointer >= len(sentence):\n",
    "#                         curr_shingle+= \"X\"*(n-len(curr_shingle))\n",
    "#                         break\n",
    "#                     elif sentence[pointer]!=' ':\n",
    "#                         curr_shingle += sentence[pointer]\n",
    "#                     else:\n",
    "#                         tag+=1\n",
    "#                     pointer += 1\n",
    "#                 head += math.ceil((tag+1)/2)\n",
    "#                 if sentence[head] == ' ':\n",
    "#                     head+=1\n",
    "#                 x.append(curr_shingle)\n",
    "#                 tags.append(tag)\n",
    "#             out_sentences.append(x)\n",
    "#             out_tags.append(tags)\n",
    "#         except:\n",
    "#             print(sentence,x,tags)\n",
    "#     return out_sentences, out_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['abs',\n",
       "  'bsc',\n",
       "  'sce',\n",
       "  'ces',\n",
       "  'ess',\n",
       "  'sso',\n",
       "  'sof',\n",
       "  'ofa',\n",
       "  'fab',\n",
       "  'abl',\n",
       "  'ble',\n",
       "  'lef',\n",
       "  'eft',\n",
       "  'ftk',\n",
       "  'tkn',\n",
       "  'kne',\n",
       "  'nee'],\n",
       " ['fac', 'aci', 'cil', 'ili', 'lit', 'ity', 'tyi', 'yid', 'id:']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b = process_sentences(['abscess of a b left knee', 'facility id:'], n =3)\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, train_tags= process_sentences(train_sentences,max_len_char)\n",
    "val_sentences, val_tags= process_sentences(val_sentences,max_len_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_dict = {}\n",
    "\n",
    "for i in train_sentences:\n",
    "    for j in i:\n",
    "        if j.lower().strip() not in word_count_dict:\n",
    "            word_count_dict[j.lower().strip()] = 1\n",
    "        else:\n",
    "            word_count_dict[j.lower().strip()] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_WORD_FREQUENCY = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = {}\n",
    "word_count = 0\n",
    "\n",
    "for i in train_sentences:\n",
    "    for j in i:\n",
    "        if j.lower().strip() not in word2id and word_count_dict[j.lower().strip()] >= MIN_WORD_FREQUENCY:\n",
    "            word2id[j.lower().strip()] = word_count\n",
    "            word_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id['<UNK>'] = len(word2id)\n",
    "word2id['<PAD>'] = len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "pickle.dump(word2id,open('meta/'+file_path+'_word2id.pkl','wb'))\n",
    "\n",
    "with open('meta/'+file_path+'_word2id.json', 'w') as outfile:\n",
    "    json.dump(word2id, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2id = {}\n",
    "tag_count = 0\n",
    "\n",
    "for i in train_tags:\n",
    "    for j in i:\n",
    "        if j not in tag2id:\n",
    "            tag2id[j] = tag_count\n",
    "            tag_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2id['<PAD>'] = len(tag2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tag2id,open('meta/'+file_path+'_tag2id.pkl','wb'))\n",
    "\n",
    "with open('meta/'+file_path+'_tag2id.json', 'w') as outfile:\n",
    "    json.dump(tag2id, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "\n",
    "for s in train_sentences:\n",
    "    tmp = []\n",
    "    for w in s:\n",
    "        if w.lower().strip() in word2id:\n",
    "            tmp.append(word2id[w.lower().strip()])\n",
    "        else:\n",
    "            tmp.append(word2id['<UNK>'])\n",
    "    X.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", truncating=\"post\", value=word2id['<PAD>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [[tag2id[w] for w in s] for s in train_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", truncating=\"post\", value=tag2id[\"<PAD>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = []\n",
    "\n",
    "for s in val_sentences:\n",
    "    tmp = []\n",
    "    for w in s:\n",
    "        if w.lower().strip() in word2id:\n",
    "            tmp.append(word2id[w.lower().strip()])\n",
    "        else:\n",
    "            tmp.append(word2id['<UNK>'])\n",
    "    X_val.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = pad_sequences(maxlen=max_len, sequences=X_val, padding=\"post\",truncating=\"post\", value = word2id['<PAD>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = []\n",
    "\n",
    "for s in val_tags:\n",
    "    tmp = []\n",
    "    for w in s:\n",
    "        if w in tag2id:\n",
    "            tmp.append(tag2id[w])\n",
    "        else:\n",
    "            tmp.append(tag2id['<PAD>'])\n",
    "    y_val.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = pad_sequences(maxlen=max_len, sequences=y_val, padding=\"post\",truncating=\"post\", value=tag2id[\"<PAD>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'D', 'K', 'N', 'P', 'U', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', 'ﾀ', 'ﾂ', 'ﾠ', 'ﾢ', 'ﾰ', 'ﾷ', 'ﾾ', '\\uffbf', 'ￂ', '￢', '\\uffef']\n"
     ]
    }
   ],
   "source": [
    "chars = set([w_i for w in word2id.keys() for w_i in w])\n",
    "n_chars = len(chars)\n",
    "print(sorted(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {c: i + 2 for i, c in enumerate(chars)}\n",
    "char2idx[\"<UNK>\"] = 1\n",
    "char2idx[\"<PAD>\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(char2idx,open('meta/'+file_path+'_char2id.pkl','wb'))\n",
    "\n",
    "with open('meta/'+file_path+'_char2id.json', 'w') as outfile:\n",
    "    json.dump(char2idx, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000 10000\n"
     ]
    }
   ],
   "source": [
    "test_sentences = open(DATA_PATH+SPECIALITY+'_test.tokens',\"r\").readlines()[:10000]\n",
    "\n",
    "print(len(test_sentences))\n",
    "test_sentences , test_tags = process_sentences(test_sentences, n= max_len_char)\n",
    "print(len(test_sentences), len(test_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load a previous trained model\n",
    "\n",
    "# word2id = pickle.load(open('../entity_tagger/meta/best_model_CharLSTM+ConjoinedWords_EM-ER_EM_ER_140420_2021-04-16 10:39:45.603327_word2id.pkl','rb'))\n",
    "# tag2id = pickle.load(open('../entity_tagger/meta/best_model_CharLSTM+ConjoinedWords_EM-ER_EM_ER_140420_2021-04-16 10:39:45.603327_tag2id.pkl','rb'))\n",
    "# char2idx = pickle.load(open('../entity_tagger/meta/best_model_CharLSTM+ConjoinedWords_EM-ER_EM_ER_140420_2021-04-16 10:39:45.603327_char2id.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "\n",
    "for s in test_sentences:\n",
    "    tmp = []\n",
    "    for w in s:\n",
    "        if w.lower().strip() in word2id:\n",
    "            tmp.append(word2id[w.lower().strip()])\n",
    "        else:\n",
    "            tmp.append(word2id['<UNK>'])\n",
    "    X_test.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pad_sequences(maxlen=max_len, sequences=X_test, padding=\"post\",truncating=\"post\", value = word2id['<PAD>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = []\n",
    "\n",
    "for s in test_tags:\n",
    "    tmp = []\n",
    "    for w in s:\n",
    "        if w in tag2id:\n",
    "            tmp.append(tag2id[w])\n",
    "        else:\n",
    "            tmp.append(tag2id['<PAD>'])\n",
    "    y_test.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = pad_sequences(maxlen=max_len, sequences=y_test, padding=\"post\",truncating=\"post\", value=tag2id[\"<PAD>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = [to_categorical(i, num_classes=len(tag2id)) for i in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X_char = []\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    sent_seq = []\n",
    "    words = sentence\n",
    "    for i in range(max_len):\n",
    "        try:\n",
    "            w = words[i].lower()            \n",
    "        except:\n",
    "            sent_seq.append([char2idx.get(\"<PAD>\")]*max_len_char)\n",
    "            continue\n",
    "        word_seq = []\n",
    "        for j in range(max_len_char):\n",
    "            try:\n",
    "                word_seq.append(char2idx.get(w[j]))\n",
    "            except:\n",
    "                word_seq.append(char2idx.get(\"<PAD>\"))\n",
    "        sent_seq.append(word_seq)\n",
    "    test_X_char.append(np.array(sent_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, X,y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        loss, acc = self.model.evaluate(self.X,self.y)\n",
    "        logs['test_loss'] = loss\n",
    "        logs['test_acc'] = acc\n",
    "        print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_model = Sequential()\n",
    "#word_model.add(Embedding(input_dim=len(word2id)+1, output_dim=30, input_length=max_len))\n",
    "\n",
    "#char_model = Sequential()\n",
    "#char_model.add(TimeDistributed(Embedding(input_dim=len(char2idx)+1, output_dim=10, input_length=max_len_char)))\n",
    "#char_model.add(TimeDistributed(LSTM(units=20, return_sequences=False, recurrent_dropout=0.2)))\n",
    "\n",
    "#final_model = Sequential()\n",
    "#final_model.add(Concatenate([word_model, char_model]))\n",
    "#final_model.add(SpatialDropout1D(0.2))\n",
    "#final_model.add(Bidirectional(LSTM(units=50, return_sequences=True, recurrent_dropout=0.2)))\n",
    "#final_model.add(TimeDistributed(Dense(len(tag2id), activation=\"softmax\")))\n",
    "\n",
    "#final_model.compile(optimizer=\"rmsprop\", loss='categorical_crossentropy', metrics=['acc'])\n",
    "#keras.utils.plot_model(final_model, \"multi_input_and_output_model-2.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.models import Functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history = final_model.fit(x=[X_train, X_char_train], y=y_train, epochs=1, batch_size=128, verbose=1)\n",
    "#history = final_model.fit(training_generator,\n",
    "#                    validation_data=validation_generator, \n",
    "#                    epochs=1,\n",
    "#                    verbose=1, callbacks = callbacks, use_multiprocessing=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-24 19:49:59.790082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-24 19:49:59.825711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-24 19:49:59.825923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-24 19:49:59.826338: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-24 19:49:59.827144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-24 19:49:59.827332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-24 19:49:59.827500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-24 19:50:00.311893: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-24 19:50:00.312098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-24 19:50:00.312262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-24 19:50:00.312387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21928 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:08:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "#from keras_contrib.layers import CRF\n",
    "\n",
    "# input and embedding for words\n",
    "word_in = Input(shape=(max_len,))\n",
    "emb_word = Embedding(input_dim=len(word2id)+1, output_dim=30,\n",
    "                     input_length=max_len)(word_in)\n",
    "\n",
    "# input and embeddings for characters\n",
    "char_in = Input(shape=(max_len, max_len_char,))\n",
    "emb_char = TimeDistributed(Embedding(input_dim=len(char2idx)+1, output_dim=10,\n",
    "                           input_length=max_len_char))(char_in)\n",
    "# character LSTM to get word encodings by characters\n",
    "char_enc = TimeDistributed(LSTM(units=20, return_sequences=False,\n",
    "                                recurrent_dropout=0.2))(emb_char)\n",
    "\n",
    "# main LSTM\n",
    "x = concatenate([emb_word, char_enc])\n",
    "x = SpatialDropout1D(0.2)(x)\n",
    "main_lstm = Bidirectional(LSTM(units=50, return_sequences=True,\n",
    "                               recurrent_dropout=0.2))(x)\n",
    "\n",
    "out = TimeDistributed(Dense(len(tag2id), activation=\"softmax\"))(main_lstm) # a dense layer as suggested by neuralNer\n",
    "\n",
    "\n",
    "\n",
    "# crf = CRF(len(tag2id))\n",
    "\n",
    "# out = crf(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=50, \n",
    "    min_lr=0.0001)\n",
    "\n",
    "tb = keras.callbacks.TensorBoard(log_dir='./logs')\n",
    "\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath='models/'+file_path+'.hdf5', monitor='val_loss', \n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 100, 4)]     0           []                               \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, 100, 4, 10)  810         ['input_2[0][0]']                \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 100, 30)      849300      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " time_distributed_1 (TimeDistri  (None, 100, 20)     2480        ['time_distributed[0][0]']       \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 100, 50)      0           ['embedding[0][0]',              \n",
      "                                                                  'time_distributed_1[0][0]']     \n",
      "                                                                                                  \n",
      " spatial_dropout1d (SpatialDrop  (None, 100, 50)     0           ['concatenate[0][0]']            \n",
      " out1D)                                                                                           \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 100, 100)     40400       ['spatial_dropout1d[0][0]']      \n",
      "                                                                                                  \n",
      " time_distributed_2 (TimeDistri  (None, 100, 3)      303         ['bidirectional[0][0]']          \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 893,293\n",
      "Trainable params: 893,293\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#from keras_contrib.losses import crf_loss\n",
    "#from keras_contrib.metrics import crf_accuracy\n",
    "\n",
    "model = Model([word_in, char_in], out)\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "#model.compile(optimizer=\"adam\", loss=crf.loss_function, metrics=[crf.accuracy])\n",
    "keras.utils.plot_model(model, \"multi_input_and_output_model.png\", show_shapes=True)\n",
    "print(model.summary())\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"configs/\"+file_path+\".json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, X, sentences,Y,tag_count, batch_size=32,shuffle=True):\n",
    "        'Initialization'\n",
    "        self.X = X\n",
    "        self.tag_count = tag_count\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        self.Y = Y\n",
    "        self.curr_index = 0\n",
    "        self.on_epoch_end()\n",
    "        self.sentences = np.array(sentences, dtype=object)\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.X)/self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]        \n",
    "        \n",
    "        X_char = []\n",
    "        \n",
    "        # for every sentence\n",
    "        for sentence in self.sentences[indexes]:\n",
    "            sent_seq = []\n",
    "            # get the words in the sentence\n",
    "            words = sentence\n",
    "            # iterate from 1 to max_len\n",
    "            for i in range(max_len):\n",
    "                # check if we have a word at i-th postion\n",
    "                try:\n",
    "                    w = words[i].lower()            \n",
    "                except:\n",
    "                    # word is not there, pad it.\n",
    "                    sent_seq.append([char2idx.get(\"<PAD>\")]*max_len_char)\n",
    "                    continue\n",
    "                \n",
    "                # iterate over 1 to 4\n",
    "                word_seq = []\n",
    "                for j in range(max_len_char):\n",
    "                    # if j-th letter is known, get the index value.\n",
    "                    try:\n",
    "                        word_seq.append(char2idx.get(w[j]))\n",
    "                    except:\n",
    "                        # for unknown letters, use the padding.\n",
    "                        word_seq.append(char2idx.get(\"<PAD>\"))\n",
    "                        \n",
    "                # add the word seq to the sent seq\n",
    "                sent_seq.append(word_seq)\n",
    "            # construct the index array for the sentence (aka X[i])    \n",
    "            X_char.append(np.array(sent_seq))\n",
    "        \n",
    "        # collect all the arrays\n",
    "        X_char = np.array(X_char)\n",
    "        \n",
    "        \n",
    "        # construct the one-hot embedding.\n",
    "        y_tmp = self.Y[indexes]\n",
    "        y_tmp = [to_categorical(i, num_classes=self.tag_count) for i in y_tmp]\n",
    "        y_tmp = np.array(y_tmp)\n",
    "        \n",
    "        #print(\"\\nindexes[1]\", indexes[1])\n",
    "        #print(\"self.X\", self.X[indexes].shape)\n",
    "        \n",
    "        #print(\"X_char\", X_char.shape)\n",
    "        #print(\"y_tmp\", np.array(y_tmp).shape)\n",
    "        \n",
    "        X_char = np.asarray(X_char).astype(np.float32)\n",
    "        y_tmp = np.asarray(y_tmp).astype(np.float32)\n",
    "        \n",
    "        #print(\" \", index)\n",
    "        return [np.asarray(self.X[indexes]).astype(np.float32), X_char], y_tmp\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.X))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence_chars(sents, c_index, max_len):\n",
    "    X_char = []\n",
    "\n",
    "    # for every sentence\n",
    "    for sentence in sents:\n",
    "        sent_seq = []\n",
    "        # get the words in the sentence\n",
    "        words = sentence\n",
    "        # iterate from 1 to max_len\n",
    "        for i in range(max_len):\n",
    "            # check if we have a word at i-th postion\n",
    "            try:\n",
    "                w = words[i].lower()            \n",
    "            except:\n",
    "                # word is not there, pad it.\n",
    "                sent_seq.append([c_index.get(\"<PAD>\")]*max_len_char)\n",
    "                continue\n",
    "\n",
    "            # iterate over 1 to 4\n",
    "            word_seq = []\n",
    "            for j in range(max_len_char):\n",
    "                # if j-th letter is known, get the index value.\n",
    "                try:\n",
    "                    word_seq.append(c_index.get(w[j]))\n",
    "                except:\n",
    "                    # for unknown letters, use the padding.\n",
    "                    word_seq.append(c_index.get(\"<PAD>\"))\n",
    "\n",
    "            # add the word seq to the sent seq\n",
    "            sent_seq.append(np.array(word_seq))\n",
    "        # construct the index array for the sentence (aka X[i])    \n",
    "        X_char.append(np.array(sent_seq))\n",
    "\n",
    "    # collect all the arrays\n",
    "    return np.array(X_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train = np.array([to_categorical(i, num_classes=len(tag2id)) for i in y])\n",
    "#X_train = X\n",
    "#X_train = np.asarray(X_train).astype(np.float32)\n",
    "#X_char_train = encode_sentence_chars(train_sentences, char2idx, max_len)\n",
    "#X_char_train = np.asarray(X_char_train).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[X_train, X_char_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 100) <dtype: 'float32'>\n",
      "(None, 100, 4) <dtype: 'float32'>\n",
      "(None, 100, 3) <dtype: 'float32'>\n",
      "input_2 [(None, 100, 4)] float32\n",
      "input_1 [(None, 100)] float32\n",
      "time_distributed (None, 100, 4) float32\n",
      "embedding (None, 100) float32\n",
      "time_distributed_1 (None, 100, 4, 10) float32\n",
      "concatenate [(None, 100, 30), (None, 100, 20)] float32\n",
      "spatial_dropout1d (None, 100, 50) float32\n",
      "bidirectional (None, 100, 50) float32\n",
      "time_distributed_2 (None, 100, 100) float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(i.shape, i.dtype) for i in model.inputs]\n",
    "[print(o.shape, o.dtype) for o in model.outputs]\n",
    "[print(l.name, l.input_shape, l.dtype) for l in model.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#callbacks = [reduce_lr,model_checkpoint,TestCallback(X_test, np.array(y_test)),tb]\n",
    "\n",
    "callbacks = [reduce_lr,model_checkpoint,tb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history = model.fit(x=[np.array(X_train), np.array(X_char_train)], y=y_train, epochs=1, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = DataGenerator(X, train_sentences, y,len(tag2id), batch_size=1024) #3000)\n",
    "validation_generator = DataGenerator(X_val, val_sentences, y_val,len(tag2id), batch_size=1024) #3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9/9 [==============================] - 10s 761ms/step - loss: 0.1101 - acc: 0.9719 - val_loss: 0.0975 - val_acc: 0.9715 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 6s 671ms/step - loss: 0.0879 - acc: 0.9786 - val_loss: 0.0785 - val_acc: 0.9784 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 6s 670ms/step - loss: 0.0699 - acc: 0.9837 - val_loss: 0.0691 - val_acc: 0.9790 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 6s 668ms/step - loss: 0.0563 - acc: 0.9866 - val_loss: 0.0564 - val_acc: 0.9832 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.0466 - acc: 0.9883"
     ]
    }
   ],
   "source": [
    "history = model.fit(training_generator,\n",
    "                    validation_data=validation_generator, \n",
    "                    epochs=10,\n",
    "                    verbose=1, callbacks = callbacks, use_multiprocessing=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"unjoiner.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'best_model_CharLSTM+ConjoinedWords_emed_EM_ED_240222_2022-11-24 19:49:56.146535'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Best Model to Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "10/10 [==============================] - 1s 67ms/step\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "#custom_objects={'CRF': CRF,'crf_loss':crf.loss_function,'crf_viterbi_accuracy':crf.accuracy}#,'crf_marginal_accuracy':crf_marginal_accuracy}\n",
    "\n",
    "model = load_model('models/'+file_path+'.hdf5')#,custom_objects=custom_objects)\n",
    "\n",
    "test_pred = model.predict([np.asarray(X_test).astype(np.float32), np.asarray(test_X_char).astype(np.float32)], verbose=1, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2tag = {i: w for w, i in tag2id.items()}\n",
    "\n",
    "def pred2label(pred):\n",
    "    out = []\n",
    "    for pred_i in pred:\n",
    "        out_i = []\n",
    "        for p in pred_i:\n",
    "            p_i = np.argmax(p)\n",
    "            out_i.append(idx2tag[p_i])\n",
    "        out.append(out_i)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_test_pred = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_pred)):\n",
    "    modified_test_pred.append(test_pred[i][:len(test_sentences[i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_test_true = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_test)):\n",
    "    modified_test_true.append(y_test[i][:len(test_sentences[i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pred2label(modified_test_pred)\n",
    "true = pred2label(modified_test_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gastrointestinal noabdominal pain,nodiarrhea ,nonausea andno"
     ]
    }
   ],
   "source": [
    "flag = False\n",
    "for i,k in enumerate(zip(test_sentences[54], pred[54])):\n",
    "    if k[1] == 1 and not Flag:\n",
    "        print(\" \",end='')\n",
    "        Flag = True\n",
    "    elif k[1] == 0:\n",
    "        Flag = False    \n",
    "    if i == 0:\n",
    "        print(k[0],end='')        \n",
    "    else:\n",
    "        print(k[0][3],end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('gast', 0)\n",
      "('astr', 0)\n",
      "('stro', 0)\n",
      "('troi', 0)\n",
      "('roin', 0)\n",
      "('oint', 0)\n",
      "('inte', 0)\n",
      "('ntes', 0)\n",
      "('test', 0)\n",
      "('esti', 0)\n",
      "('stin', 0)\n",
      "('tina', 0)\n",
      "('inal', 0)\n",
      "('naln', 1)\n",
      "('alno', 1)\n",
      "('lnoa', 1)\n",
      "('noab', 1)\n",
      "('oabd', 1)\n",
      "('abdo', 0)\n",
      "('bdom', 0)\n",
      "('domi', 0)\n",
      "('omin', 0)\n",
      "('mina', 0)\n",
      "('inal', 0)\n",
      "('nalp', 1)\n",
      "('alpa', 1)\n",
      "('lpai', 1)\n",
      "('pain', 1)\n",
      "('ain,', 1)\n",
      "('in,n', 1)\n",
      "('n,no', 1)\n",
      "(',nod', 1)\n",
      "('nodi', 1)\n",
      "('odia', 1)\n",
      "('diar', 0)\n",
      "('iarr', 0)\n",
      "('arrh', 0)\n",
      "('rrhe', 0)\n",
      "('rhea', 0)\n",
      "('hea,', 1)\n",
      "('ea,n', 1)\n",
      "('a,no', 1)\n",
      "(',non', 1)\n",
      "('nona', 1)\n",
      "('onau', 1)\n",
      "('naus', 0)\n",
      "('ause', 0)\n",
      "('usea', 0)\n",
      "('seaa', 1)\n",
      "('eaan', 1)\n",
      "('aand', 1)\n",
      "('andn', 1)\n",
      "('ndno', 1)\n"
     ]
    }
   ],
   "source": [
    "for i,k in enumerate(zip(test_sentences[54], pred[54])):\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/'+file_path+'_list_true.txt','w') as f:\n",
    "    for i in true:\n",
    "        for j in i:\n",
    "            f.write(str(j)+\" \")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/'+file_path+'_list_pred.txt','w') as f:\n",
    "    for i in pred:\n",
    "        for j in i:\n",
    "            f.write(str(j)+\" \")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/'+file_path+'_list_shingles.txt','w') as f:\n",
    "    for i in test_sentences:\n",
    "        for j in i:\n",
    "            f.write(str(j)+\" \")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = open(DATA_PATH+SPECIALITY+'_test.tokens',\"r\").readlines()[:10000]\n",
    "\n",
    "with open('data/'+file_path+'_sentences.txt','w') as f:\n",
    "    for i in test_sentences:\n",
    "        for j in i.split(\" \"):\n",
    "            f.write(str(j)+\" \")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "y_true = []\n",
    "y_pred = []\n",
    "pred = open('data/'+file_path+'_list_pred.txt',\"r\")\n",
    "\n",
    "with open('data/'+file_path+'_list_true.txt',\"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        labels = pred.readline().split()\n",
    "        for i in range(len(line)):\n",
    "            if i < len(labels):\n",
    "                y_pred+=[labels[i]]\n",
    "            else:\n",
    "                y_pred+=[\"o\"]\n",
    "            y_true+=[line[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sudarsun/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sudarsun/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sudarsun/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_true, y_pred,output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(report).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('reports/'+file_path+'_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.910071</td>\n",
       "      <td>0.869025</td>\n",
       "      <td>0.889075</td>\n",
       "      <td>145051.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.894046</td>\n",
       "      <td>0.926196</td>\n",
       "      <td>0.909837</td>\n",
       "      <td>172971.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;PAD&gt;</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.900120</td>\n",
       "      <td>0.900120</td>\n",
       "      <td>0.900120</td>\n",
       "      <td>0.90012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.601372</td>\n",
       "      <td>0.598407</td>\n",
       "      <td>0.599637</td>\n",
       "      <td>318022.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.901355</td>\n",
       "      <td>0.900120</td>\n",
       "      <td>0.900367</td>\n",
       "      <td>318022.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score       support\n",
       "0              0.910071  0.869025  0.889075  145051.00000\n",
       "1              0.894046  0.926196  0.909837  172971.00000\n",
       "<PAD>          0.000000  0.000000  0.000000       0.00000\n",
       "accuracy       0.900120  0.900120  0.900120       0.90012\n",
       "macro avg      0.601372  0.598407  0.599637  318022.00000\n",
       "weighted avg   0.901355  0.900120  0.900367  318022.00000"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_df = pd.DataFrame(confusion_matrix(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_899637/3738547203.py:1: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  cm_df.rows = sorted(unique_labels(y_true, y_pred))\n"
     ]
    }
   ],
   "source": [
    "cm_df.rows = sorted(unique_labels(y_true, y_pred))\n",
    "cm_df.columns = sorted(unique_labels(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_df.to_csv('reports/'+file_path+'_confusion_matrix.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag level analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {i: w for w, i in word2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = open(DATA_PATH+SPECIALITY+'_test.tokens',\"r\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [97], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m true \u001b[38;5;241m=\u001b[39m modified_test_true[i]\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(idx):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mpredicted\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m!=\u001b[39m true[j]:\n\u001b[1;32m     14\u001b[0m         count\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     15\u001b[0m         error \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m4\u001b[39m,),dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "errors = []\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    words = test_sentences[i].strip().split(' ')[:50]\n",
    "    try:\n",
    "        idx = words.index('<PAD>')\n",
    "    except:\n",
    "        idx = len(words)\n",
    "    predicted = modified_test_pred[i].argmax(axis=1)\n",
    "    true = modified_test_true[i].argmax(axis=1)\n",
    "    for j in range(idx):\n",
    "        if predicted[j] != true[j]:\n",
    "            count+=1\n",
    "            error = np.zeros((4,),dtype=object)\n",
    "            str1 = ' '.join(str(e) for e in words[:idx])\n",
    "            error[0] = (str1)\n",
    "            error[1] = (words[j])\n",
    "            error[2] = (idx2tag[true[j]])\n",
    "            error[3] = (idx2tag[predicted[j]])\n",
    "            errors.append(error)\n",
    "print (count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_df = pd.DataFrame(errors)\n",
    "errors_df.columns = ['Sentence','Word', 'Ground Truth', 'Predicted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import ExcelWriter\n",
    "\n",
    "for i in range(0,len(errors_df),100000):\n",
    "    writer = ExcelWriter('reports/'+file_path+'_test_errors_keywords'+str(i)+'.xlsx')\n",
    "    errors_df[i:min(i+100000,len(errors_df))].to_excel(writer,index=False)\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
