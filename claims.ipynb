{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import functools\n",
    "from sklearn import tree, model_selection\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "import gc\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree, export_text\n",
    "import sys\n",
    "from sklearn.tree import _tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class colors:\n",
    "    '''Colors class:reset all colors with colors.reset; two\n",
    "    sub classes fg for foreground\n",
    "    and bg for background; use as colors.subclass.colorname.\n",
    "    i.e. colors.fg.red or colors.bg.greenalso, the generic bold, disable,\n",
    "    underline, reverse, strike through,\n",
    "    and invisible work with the main class i.e. colors.bold'''\n",
    "    reset = '\\033[0m'\n",
    "    bold = '\\033[01m'\n",
    "    disable = '\\033[02m'\n",
    "    underline = '\\033[04m'\n",
    "    reverse = '\\033[07m'\n",
    "    strikethrough = '\\033[09m'\n",
    "    invisible = '\\033[08m'\n",
    "\n",
    "    class fg:\n",
    "        black = '\\033[30m'\n",
    "        red = '\\033[31m'\n",
    "        green = '\\033[32m'\n",
    "        orange = '\\033[33m'\n",
    "        blue = '\\033[34m'\n",
    "        purple = '\\033[35m'\n",
    "        cyan = '\\033[36m'\n",
    "        lightgrey = '\\033[37m'\n",
    "        darkgrey = '\\033[90m'\n",
    "        lightred = '\\033[91m'\n",
    "        lightgreen = '\\033[92m'\n",
    "        yellow = '\\033[93m'\n",
    "        lightblue = '\\033[94m'\n",
    "        pink = '\\033[95m'\n",
    "        lightcyan = '\\033[96m'\n",
    "\n",
    "    class bg:\n",
    "        black = '\\033[40m'\n",
    "        red = '\\033[41m'\n",
    "        green = '\\033[42m'\n",
    "        orange = '\\033[43m'\n",
    "        blue = '\\033[44m'\n",
    "        purple = '\\033[45m'\n",
    "        cyan = '\\033[46m'\n",
    "        lightgrey = '\\033[47m'\n",
    "\n",
    "# without this fix, the function fails with stack overflow exception.        \n",
    "sys.setrecursionlimit(5000)\n",
    "def get_rules(tree, feature_names, class_names):\n",
    "    tree_ = tree.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "\n",
    "    paths = []\n",
    "    path = []\n",
    "    \n",
    "    def recurse(node, path, paths):\n",
    "        \n",
    "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            p1, p2 = list(path), list(path)\n",
    "            p1 += [f\"({name} <= {np.round(threshold, 3)})\"]\n",
    "            recurse(tree_.children_left[node], p1, paths)\n",
    "            p2 += [f\"({name} > {np.round(threshold, 3)})\"]\n",
    "            recurse(tree_.children_right[node], p2, paths)\n",
    "        else:\n",
    "            path += [(tree_.value[node], tree_.n_node_samples[node])]\n",
    "            paths += [path]\n",
    "            \n",
    "    recurse(0, path, paths)\n",
    "\n",
    "    # sort by samples count\n",
    "    samples_count = [p[-1][1] for p in paths]\n",
    "    ii = list(np.argsort(samples_count))\n",
    "    paths = [paths[i] for i in reversed(ii)]\n",
    "    \n",
    "    rules = []\n",
    "    for path in paths:\n",
    "        rule = \"if \"\n",
    "        \n",
    "        for p in path[:-1]:\n",
    "            if rule != \"if \":\n",
    "                rule += \" and \"\n",
    "            rule += str(p)\n",
    "        rule += \" then \"\n",
    "        if class_names is None:\n",
    "            rule += \"response: \"+str(np.round(path[-1][0][0][0],3))\n",
    "        else:\n",
    "            classes = path[-1][0][0]\n",
    "            l = np.argmax(classes)\n",
    "            rule += f\"class: {class_names[l]} (proba: {np.round(100.0*classes[l]/np.sum(classes),2)}%)\"\n",
    "        rule += f\" | based on {path[-1][1]:,} samples\"\n",
    "        rules += [rule]\n",
    "        \n",
    "    return rules\n",
    "\n",
    "def uniform_prior(prior):\n",
    "    result = prior.copy()\n",
    "    spread = 1./len(prior)\n",
    "    for k in prior.keys():\n",
    "        result[k] = spread\n",
    "    return result\n",
    "\n",
    "# function to fix the prior prob of PASS class, the rest of the classes shall have uniform prob.\n",
    "def fix_PASS_prior(prior, value):\n",
    "    result = prior.copy()\n",
    "    remain = (1 - value)/(len(prior)-1)\n",
    "    for k in prior.keys():\n",
    "        if k == \"PASS\":\n",
    "            result[k] = value\n",
    "        else:\n",
    "            result[k] = remain\n",
    "    return result\n",
    "\n",
    "def encode_column(coldata, index):\n",
    "    encoded = []\n",
    "    for r in coldata:\n",
    "        # check for nan by doing this comparison.\n",
    "        if r != r:\n",
    "            encoded.append([])\n",
    "        else:\n",
    "            # first filter the invalid codes from the list.\n",
    "            valid_codes = list(filter(lambda x: x in index, str(r).split(\",\")))\n",
    "            # map the codes to the index values.\n",
    "            codes = list(map(lambda x: index[x], valid_codes)) #str(r).split(\",\")))\n",
    "            encoded.append(codes)\n",
    "    return encoded\n",
    "\n",
    "def transform_denial_code(c, denial_index):\n",
    "    if c != c:\n",
    "        return 0\n",
    "    return denial_index[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(r'/home/sudarsun/notebooks/Sudar_sir_claims_scrubber_data_historical_v1.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['claim_id', 'patient_id', 'payer_id', 'plan_name', 'denial_code',\n",
       "       'code_activity', 'activity_desc', 'type_activity', 'act_type_dsc',\n",
       "       'pdx', 'sdx', 'Reason_for_visit', 'consolidated_diagnoses'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrecords = len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASS         478574\n",
      "MNEC-004     102590\n",
      "PRCE-002       7419\n",
      "PRCE-001       7305\n",
      "CODE-010       4061\n",
      "MNEC-005       3894\n",
      "CLAI-012       1827\n",
      "NCOV-0026      1518\n",
      "DUPL-002       1413\n",
      "PRCE-006        609\n",
      "COPY-001        400\n",
      "NCOV-001        337\n",
      "AUTH-001        308\n",
      "PRCE-010        286\n",
      "CODE-014        245\n",
      "ELIG-006        216\n",
      "NCOV-003        213\n",
      "ELIG-001        210\n",
      "PRCE-007        186\n",
      "TIME-001        178\n",
      "CLAI-008        117\n",
      "ELIG-007        116\n",
      "AUTH-003        105\n",
      "CLAI-016         62\n",
      "BENX-002         59\n",
      "BENX-005         57\n",
      "AUTH-005         50\n",
      "ELIG-005         43\n",
      "MNEC-003         42\n",
      "AUTH-004         29\n",
      "DUPL-001          2\n",
      "CLAI-018          1\n",
      "Name: denial_code, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# handle NAs in the denial code outputs\n",
    "data[\"denial_code\"] = data[\"denial_code\"].fillna(\"PASS\")\n",
    "dc_distrib = data[\"denial_code\"].value_counts()\n",
    "print(dc_distrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels found ['PASS' 'MNEC-004' 'PRCE-010' 'PRCE-002' 'CLAI-012' 'MNEC-005' 'CODE-010'\n",
      " 'DUPL-002' 'PRCE-001' 'PRCE-006' 'CODE-014' 'AUTH-001' 'AUTH-003'\n",
      " 'CLAI-016' 'PRCE-007' 'NCOV-003' 'NCOV-001' 'AUTH-005' 'ELIG-005'\n",
      " 'ELIG-001' 'ELIG-007' 'ELIG-006' 'CLAI-008' 'MNEC-003' 'AUTH-004'\n",
      " 'NCOV-0026' 'TIME-001' 'BENX-002' 'COPY-001' 'BENX-005' 'CLAI-018'\n",
      " 'DUPL-001']\n"
     ]
    }
   ],
   "source": [
    "denial_codes = pd.unique(data[\"denial_code\"])\n",
    "print(\"labels found\", denial_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the required denial codes of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class labels of interest ['PASS', 'MNEC-004']\n"
     ]
    }
   ],
   "source": [
    "# set the classes of interest.\n",
    "#denial_codes = list(dc_distrib.keys())\n",
    "denial_codes = [\"PASS\", \"MNEC-004\"]\n",
    "print('class labels of interest', denial_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter the cpt codes by some constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# either select all the codes, \n",
    "selected_cpts = data[\"code_activity\"]\n",
    "\n",
    "# OR filter the CPT codes that are starting with \"8\"\n",
    "#selected_cpts = []\n",
    "#for code in data[\"code_activity\"]:\n",
    "#    if str(code)[0] == '8':\n",
    "#        selected_cpts.append(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique CPT codes -> [76700 99283 'I88-4273-03444-01' ... 82384 'C15-1794-00146-01' 73706]\n",
      "# unique CPTs:  1574\n"
     ]
    }
   ],
   "source": [
    "# we are strongly assuming that the data is not multi-label.\n",
    "# get the unique cpt codes\n",
    "#cpt = pd.unique(list(data[\"code_activity\"]))\n",
    "cpt = pd.unique(selected_cpts)\n",
    "print(\"unique CPT codes ->\", cpt)\n",
    "print(\"# unique CPTs: \", len(cpt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear previously owned\n",
    "if \"pdx_icds\" in locals():\n",
    "    # release the memory blocks\n",
    "    del(pdx_icds)\n",
    "    del(sdx_icds)\n",
    "    del(rov_icds)\n",
    "    gc.collect()\n",
    "    \n",
    "# collect the icd codes from the diagnosis columns\n",
    "pdx_icds = [str(code).split(',') for code in data[data[\"pdx\"].notna()][\"pdx\"]]\n",
    "sdx_icds = [str(code).split(',') for code in data[data[\"sdx\"].notna()][\"sdx\"]] \n",
    "rov_icds = [str(code).split(',') for code in data[data[\"Reason_for_visit\"].notna()][\"Reason_for_visit\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the pdx items\n",
    "pdx = []\n",
    "for row in pdx_icds:\n",
    "    for code in row:\n",
    "        pdx.append(code)\n",
    "# collect the sdx items\n",
    "sdx = []\n",
    "for row in sdx_icds:\n",
    "    for code in row:\n",
    "        sdx.append(code)\n",
    "# collect the rov items        \n",
    "rov = []\n",
    "for row in rov_icds:\n",
    "    for code in row:\n",
    "        rov.append(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set the ICD min support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique icd codes -> ['E55.9' 'R53.83' 'D50.9' ... 'H01.131' 'T23.212A' 'S56.424D']\n",
      "# unique icds:  5337\n"
     ]
    }
   ],
   "source": [
    "# identify the codes that does not have the min-support\n",
    "icd_minsupport = 5\n",
    "\n",
    "# compute the ICD distribution\n",
    "icd_distrib = pd.DataFrame(pdx + sdx + rov).value_counts()\n",
    "\n",
    "# now filter the ICD codes that don't have the required support.\n",
    "icd_to_retain = np.array(list(filter(lambda x: icd_distrib[x] >= icd_minsupport, icd_distrib.keys()))).flatten()\n",
    "\n",
    "print(\"unique icd codes ->\", icd_to_retain)\n",
    "print(\"# unique icds: \", len(icd_to_retain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward and reverse index the unique codes.\n",
    "icd_indices = dict((i,c) for c,i in enumerate(icd_to_retain))\n",
    "cpt_indices = dict((str(c),i) for i, c in enumerate(cpt))\n",
    "denial_indices = dict((str(c),i) for i, c in enumerate(denial_codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PASS': 0, 'MNEC-004': 1}\n",
      "['PASS', 'MNEC-004']\n"
     ]
    }
   ],
   "source": [
    "print(denial_indices)\n",
    "print(denial_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the type activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_activity = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Also filter by CPT codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASS        1915\n",
      "MNEC-004     238\n",
      "Name: denial_code, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/store/userdata/sudarsun/claims/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \n",
      "/mnt/store/userdata/sudarsun/claims/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "filter_by_8xxxx_codes = False\n",
    "\n",
    "# construct a new data frame with only the necessary data.\n",
    "dc_array = denial_codes.copy()\n",
    "# also limit the scope of only CPT codes, type_activity=3\n",
    "new_data = data[data[\"denial_code\"] == dc_array[0]][data[\"type_activity\"]==type_activity]\n",
    "\n",
    "# FILTER BY 8XXXX CPT codes\n",
    "if filter_by_8xxxx_codes:\n",
    "    new_data = new_data[new_data[\"code_activity\"] < 90000][new_data[\"code_activity\"] >= 80000]\n",
    "    \n",
    "dc_array.pop(0)\n",
    "for dc in dc_array:\n",
    "    mydf = data[data[\"denial_code\"] == dc][data[\"type_activity\"]==type_activity]\n",
    "    \n",
    "    # FILTER BY 8XXXX CPT codes\n",
    "    if filter_by_8xxxx_codes:\n",
    "        mydf = mydf[mydf[\"code_activity\"] < 90000][mydf[\"code_activity\"] >= 80000]\n",
    "        \n",
    "    new_data = new_data.append(mydf)\n",
    "print(new_data[\"denial_code\"].value_counts())\n",
    "\n",
    "# update the no of records.\n",
    "nrecords = len(new_data)\n",
    "\n",
    "# @TODO we should destroy the original data frame for memory saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASS        0.889457\n",
      "MNEC-004    0.110543\n",
      "Name: denial_code, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# encode the claim result\n",
    "#claim_status = new_data[\"denial_code\"] == \"PASS\"\n",
    "#let's first compute the class prior\n",
    "#prior = claim_status.value_counts(normalize=True)\n",
    "\n",
    "prior = new_data[\"denial_code\"].value_counts(dropna=False, normalize=True)\n",
    "print(prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the training and testing priors as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Required test prior:\n",
      " PASS        0.889457\n",
      "MNEC-004    0.110543\n",
      "Name: denial_code, dtype: float64\n",
      "\n",
      "Required train prior:\n",
      " PASS        0.8\n",
      "MNEC-004    0.2\n",
      "Name: denial_code, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# set the required testing data prior\n",
    "test_prior = prior\n",
    "#test_prior = uniform_prior(prior)\n",
    "print(\"\\nRequired test prior:\\n\",test_prior)\n",
    "\n",
    "# override the train prior if needed\n",
    "#train_prior = prior\n",
    "#train_prior = uniform_prior(prior)\n",
    "train_prior = fix_PASS_prior(prior, 0.8)\n",
    "print(\"\\nRequired train prior:\\n\", train_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set the required training and testing corpus sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the number of required training & testing sample sizes\n",
    "ntrain = 1150\n",
    "ntest = 2\n",
    "#ntrain = 307000\n",
    "#ntest = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASS        1915\n",
      "MNEC-004     238\n",
      "Name: denial_code, dtype: int64\n",
      "\u001b[34m class: PASS available= 1915 req_train= 920 req_test= 1\n",
      "\u001b[34m class: MNEC-004 available= 238 req_train= 229 req_test= 0\n"
     ]
    }
   ],
   "source": [
    "available = new_data[\"denial_code\"].value_counts()\n",
    "print(available)\n",
    "for dc in denial_codes:\n",
    "    av = available[dc]\n",
    "    tr = int(train_prior[dc]*ntrain)\n",
    "    te = int(test_prior[dc]*ntest)\n",
    "    print(colors.fg.blue, \"class:\", dc, \"available=\", av, \"req_train=\", tr, \"req_test=\", te)\n",
    "    if (tr+te) > av:\n",
    "        print(colors.bold, colors.fg.red, \"\\nPROBLEM: class\", dc, \"required\", tr+te, \"> available\",av,\"please adjust ntrain and ntest\", colors.reset)\n",
    "        sys.exit(1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASS --> available points: 1915 || required train points: 920 || test points: 1\n",
      "got 920 train & 1 test points.\n",
      "MNEC-004 --> available points: 238 || required train points: 229 || test points: 0\n",
      "got 229 train & 0 test points.\n",
      "\n",
      "Required train data prior:\n",
      " PASS        0.8\n",
      "MNEC-004    0.2\n",
      "Name: denial_code, dtype: float64\n",
      "\n",
      "Actual train data prior:\n",
      " PASS        0.800696\n",
      "MNEC-004    0.199304\n",
      "Name: denial_code, dtype: float64\n",
      "\n",
      "Required test data prior:\n",
      " PASS        0.889457\n",
      "MNEC-004    0.110543\n",
      "Name: denial_code, dtype: float64\n",
      "\n",
      "Actual test data prior:\n",
      " PASS    1.0\n",
      "Name: denial_code, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# now collect the ids for training and testing samples.\n",
    "train_ids = []    \n",
    "test_ids = []\n",
    "\n",
    "cum_train = 0\n",
    "cum_test = 0\n",
    "# now, we have to draw the sample from tr_indexed data points, hopefully following the class prior.\n",
    "for c in prior.keys():\n",
    "    # get the required training data count\n",
    "    req_ntrain = int(train_prior[c] * ntrain)\n",
    "    req_ntest = int(test_prior[c] * ntest)\n",
    "    \n",
    "    # get the keys from the data\n",
    "    #keys = data[data[\"denial_code\"]==c][\"denial_code\"].keys()\n",
    "    keys = new_data[new_data[\"denial_code\"]==c][\"denial_code\"].keys()\n",
    "    print(c, \"--> available points:\", len(keys), \"|| required train points:\", req_ntrain, \"|| test points:\", req_ntest)\n",
    "\n",
    "    # permutate the keys.\n",
    "    keys = np.random.permutation(keys)\n",
    "    \n",
    "    # if the required count is less than available, slice the array into train and test.\n",
    "    if len(keys) > req_ntrain + req_ntest:\n",
    "        train_ids = train_ids + list(keys[0:req_ntrain])\n",
    "        test_ids = test_ids + list(keys[req_ntrain:(req_ntrain+req_ntest)])\n",
    "    else:\n",
    "        print(colors.bold, colors.fg.red, \"\\nPROBLEM: required samples greater than available, please adjust ntrain and ntest\", colors.reset)\n",
    "        break\n",
    "    \n",
    "    #for k in keys:\n",
    "    #    if k in tr_index and len(train_ids)-cum_train < req_ntrain:\n",
    "    #        train_ids.append(k)\n",
    "    #    if k in te_index and len(test_ids)-cum_test < req_ntest:\n",
    "    #        test_ids.append(k)\n",
    "    print(\"got\", len(train_ids)-cum_train, \"train &\", len(test_ids)-cum_test, \"test points.\")\n",
    "    cum_train = len(train_ids)\n",
    "    cum_test = len(test_ids)\n",
    "\n",
    "print(\"\\nRequired train data prior:\\n\", train_prior)\n",
    "print(\"\\nActual train data prior:\\n\", new_data[\"denial_code\"][train_ids].value_counts(normalize=True))\n",
    "print(\"\\nRequired test data prior:\\n\", test_prior)\n",
    "print(\"\\nActual test data prior:\\n\", new_data[\"denial_code\"][test_ids].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pop=PASS~920,MNEC-004~229~~tr=PASS~0.8,MNEC-004~0.2~~val=PASS~1.0\n"
     ]
    }
   ],
   "source": [
    "# create the model name based on the settings.\n",
    "distrib = new_data[\"denial_code\"][train_ids].value_counts()\n",
    "part1 = \",\".join(map(lambda x: x + \"~\" + str(distrib[x]), distrib.keys()))\n",
    "tr_distrib = new_data[\"denial_code\"][train_ids].value_counts(normalize=True)\n",
    "part2 = \",\".join(map(lambda x: x + \"~\" + str(round(tr_distrib[x],2)), tr_distrib.keys()))\n",
    "te_distrib = new_data[\"denial_code\"][test_ids].value_counts(normalize=True)\n",
    "part3 = \",\".join(map(lambda x: x + \"~\" + str(round(te_distrib[x],2)), te_distrib.keys()))\n",
    "model_name = \"pop=\" + part1 + \"~~tr=\" + part2 + \"~~val=\" + part3\n",
    "\n",
    "if filter_by_8xxxx_codes:\n",
    "    model_name += \"~~cpt=8XXXX\"\n",
    "\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode the PDX column\n",
    "pdx_encoded_tr = encode_column(new_data[\"pdx\"][train_ids], icd_indices)\n",
    "pdx_encoded_te = encode_column(new_data[\"pdx\"][test_ids], icd_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode the SDX column\n",
    "sdx_encoded_tr = encode_column(new_data[\"sdx\"][train_ids], icd_indices)\n",
    "sdx_encoded_te = encode_column(new_data[\"sdx\"][test_ids], icd_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the ROV column\n",
    "rov_encoded_tr = encode_column(new_data[\"Reason_for_visit\"][train_ids], icd_indices)\n",
    "rov_encoded_te = encode_column(new_data[\"Reason_for_visit\"][test_ids], icd_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the CPT column\n",
    "cpt_encoded_tr = encode_column(new_data[\"code_activity\"][train_ids], cpt_indices)\n",
    "cpt_encoded_te = encode_column(new_data[\"code_activity\"][test_ids], cpt_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the denial column\n",
    "denial_encoded_tr = list(map(lambda x: transform_denial_code(x, denial_indices), new_data[\"denial_code\"][train_ids]))\n",
    "denial_encoded_te = list(map(lambda x: transform_denial_code(x, denial_indices), new_data[\"denial_code\"][test_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(record, num_classes):\n",
    "    encoded = np.zeros(num_classes)\n",
    "    for r in record:\n",
    "        encoded[r] = 1.\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free up the space explicitly\n",
    "if 'X_train' in locals():\n",
    "    del(X_train) \n",
    "    del(X_test)\n",
    "    gc.collect()\n",
    "\n",
    "X_train = [np.hstack((one_hot(cpt_encoded_tr[i], num_classes=len(cpt_indices)), \n",
    "                      one_hot(rov_encoded_tr[i], num_classes=len(icd_indices)),\n",
    "                      one_hot(sdx_encoded_tr[i], num_classes=len(icd_indices)),\n",
    "                      one_hot(pdx_encoded_tr[i], num_classes=len(icd_indices)))) for i in range(len(train_ids))]\n",
    "X_test  = [np.hstack((one_hot(cpt_encoded_te[i], num_classes=len(cpt_indices)), \n",
    "                      one_hot(rov_encoded_te[i], num_classes=len(icd_indices)),\n",
    "                      one_hot(sdx_encoded_te[i], num_classes=len(icd_indices)),\n",
    "                      one_hot(pdx_encoded_te[i], num_classes=len(icd_indices)))) for i in range(len(test_ids))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17585\n"
     ]
    }
   ],
   "source": [
    "# free up the space explicitly\n",
    "if 'features' in locals():\n",
    "    del(features)\n",
    "    gc.collect()\n",
    "\n",
    "features = list(cpt_indices.keys()) + [\"RFV-\"+k for k in icd_indices.keys()] + [\"SDX-\"+k for k in icd_indices.keys()] + [\"PDX-\"+k for k in icd_indices.keys()]\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pop=PASS~920,MNEC-004~229~~tr=PASS~0.8,MNEC-004~0.2~~val=PASS~1.0~~n_features=17585~~type-activity=8\n"
     ]
    }
   ],
   "source": [
    "model_name += \"~~n_features=\" + str(len(features))\n",
    "model_name += \"~~type-activity=\" + str(type_activity)\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m MODEL file pop=PASS~920,MNEC-004~229~~tr=PASS~0.8,MNEC-004~0.2~~val=PASS~1.0~~n_features=17585~~type-activity=8.pkl is not available, building it.. \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "node_minsup = 10\n",
    "model_name += \"~~minsup=\" + str(node_minsup)\n",
    "\n",
    "model_already_available = True\n",
    "#check if the file is already available, if so, load it.\n",
    "try:\n",
    "    with open(model_name + \".pkl\", \"rb\") as model_fp:\n",
    "        print(colors.fg.blue, \"MODEL file\", model_name + \".pkl\", \"is already available, loading it..\" ,colors.reset)\n",
    "        dt = pickle.load(model_fp)\n",
    "        model_fp.close()\n",
    "except FileNotFoundError:\n",
    "    print(colors.fg.red, \"MODEL file\", model_name + \".pkl\", \"is not available, building it..\" ,colors.reset)\n",
    "    # if the file is not present, let's create the model freshly.\n",
    "    model_already_available = False\n",
    "    dt = tree.DecisionTreeClassifier(min_samples_leaf=node_minsup, class_weight=\"balanced\")\n",
    "    dt.fit(X_train, denial_encoded_tr)\n",
    "    # save the model.\n",
    "    with open(model_name + \".pkl\", \"wb\") as file:\n",
    "        pickle.dump(dt, file)\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's run the prediction only if the model is built from scratch\n",
    "if model_already_available == False:\n",
    "    y_pred = dt.predict(X_test)\n",
    "    y_hat_train = dt.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       920\n",
      "           1       0.91      0.98      0.95       229\n",
      "\n",
      "    accuracy                           0.98      1149\n",
      "   macro avg       0.95      0.98      0.97      1149\n",
      "weighted avg       0.98      0.98      0.98      1149\n",
      "\n",
      "Accuracy = 0.9773716275021758\n"
     ]
    }
   ],
   "source": [
    "# let's run the prediction only if the model is built from scratch\n",
    "if model_already_available == False:\n",
    "    print(classification_report(denial_encoded_tr, y_hat_train, zero_division=0))\n",
    "    print(\"Accuracy =\", accuracy_score(denial_encoded_tr, y_hat_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         1\n",
      "   macro avg       1.00      1.00      1.00         1\n",
      "weighted avg       1.00      1.00      1.00         1\n",
      "\n",
      "Test Accuracy = 1.0\n"
     ]
    }
   ],
   "source": [
    "# let's run the prediction only if the model is built from scratch\n",
    "if model_already_available == False:\n",
    "    #cm = confusion_matrix(y_test, y_pred)\n",
    "    cm = confusion_matrix(denial_encoded_te, y_pred)\n",
    "    #print(cm)\n",
    "\n",
    "    print(classification_report(denial_encoded_te, y_pred, zero_division=0))\n",
    "    print(\"Test Accuracy =\", accuracy_score(denial_encoded_te, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model.\n",
    "#with open(model_name, \"wb\") as file:\n",
    "#    pickle.dump(dt, file)\n",
    "#    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf = RandomForestClassifier(n_jobs=10)\n",
    "#rf.fit(X_train, denial_encoded_tr)\n",
    "#rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yhat = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(100,40))\n",
    "#plot_tree(dt, filled=True, rounded=True, impurity=True, feature_names=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the rules now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extract the induced rules and save them to a file.\n",
    "rules = get_rules(dt, features, denial_codes)\n",
    "rule_text = \"\\n\".join(rules)\n",
    "\n",
    "with open(model_name + \".rules\", \"wt\") as rule_file:\n",
    "    rule_file.write(rule_text)\n",
    "    rule_file.close()\n",
    "    \n",
    "    #for r in rules:\n",
    "        #print(r)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
