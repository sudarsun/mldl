{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Activation, LSTM, RepeatVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Go.' 'Va !'\n",
      "  'CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)']\n",
      " ['Hi.' 'Salut !'\n",
      "  'CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #509819 (Aiji)']\n",
      " ['Hi.' 'Salut.'\n",
      "  'CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #4320462 (gillux)']\n",
      " ...\n",
      " [\"Death is something that we're often discouraged to talk about or even think about, but I've realized that preparing for death is one of the most empowering things you can do. Thinking about death clarifies your life.\"\n",
      "  \"La mort est une chose qu'on nous décourage souvent de discuter ou même de penser mais j'ai pris conscience que se préparer à la mort est l'une des choses que nous puissions faire qui nous investit le plus de responsabilité. Réfléchir à la mort clarifie notre vie.\"\n",
      "  'CC-BY 2.0 (France) Attribution: tatoeba.org #1969892 (davearms) & #1969962 (sacredceltic)']\n",
      " ['Since there are usually multiple websites on any given topic, I usually just click the back button when I arrive on any webpage that has pop-up advertising. I just go to the next page found by Google and hope for something less irritating.'\n",
      "  \"Puisqu'il y a de multiples sites web sur chaque sujet, je clique d'habitude sur le bouton retour arrière lorsque j'atterris sur n'importe quelle page qui contient des publicités surgissantes. Je me rends juste sur la prochaine page proposée par Google et espère tomber sur quelque chose de moins irritant.\"\n",
      "  'CC-BY 2.0 (France) Attribution: tatoeba.org #954270 (CK) & #957693 (sacredceltic)']\n",
      " [\"If someone who doesn't know your background says that you sound like a native speaker, it means they probably noticed something about your speaking that made them realize you weren't a native speaker. In other words, you don't really sound like a native speaker.\"\n",
      "  \"Si quelqu'un qui ne connaît pas vos antécédents dit que vous parlez comme un locuteur natif, cela veut dire qu'il a probablement remarqué quelque chose à propos de votre élocution qui lui a fait prendre conscience que vous n'êtes pas un locuteur natif. En d'autres termes, vous ne parlez pas vraiment comme un locuteur natif.\"\n",
      "  'CC-BY 2.0 (France) Attribution: tatoeba.org #953936 (CK) & #955961 (sacredceltic)']]\n",
      "overall pairs 175623\n"
     ]
    }
   ],
   "source": [
    "raw_data = open(r\"/home/sudarsun/projects/ML_DL_py_TF/Chapter12_RNN_LSTM_V3/Datasets/fra-eng/fra.txt\", mode='rt', encoding='utf-8').read()\n",
    "raw_data = raw_data.strip().split(\"\\n\")\n",
    "raw_data = [i.split('\\t') for i in raw_data]\n",
    "data = np.array(raw_data)\n",
    "print(data)\n",
    "print(\"overall pairs\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(175623, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:,0] = [word.translate(str.maketrans('', '', string.punctuation)) for word in data[:,0]]\n",
    "data[:,1] = [word.translate(str.maketrans('', '', string.punctuation)) for word in data[:,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in range(len(data)):\n",
    "    data[word,0] = data[word,0].lower()\n",
    "    data[word,1] = data[word,1].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lang 1 vocab size 14671\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data[:,0])\n",
    "l1_tokens = tokenizer\n",
    "l1_vocab_size = len(l1_tokens.word_index) + 1\n",
    "print(\"lang 1 vocab size\", l1_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lang 2 vocab size 33321\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data[:,1])\n",
    "l2_tokens = tokenizer\n",
    "l2_vocab_size = len(l2_tokens.word_index) + 1\n",
    "print(\"lang 2 vocab size\", l2_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (158060, 15)\n",
      "Y_train.shape (158060, 15)\n",
      "X_test.shape (17563, 15)\n",
      "Y_test.shape (17563, 15)\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(data, test_size=0.1, random_state=43)\n",
    "X_train_seq = l1_tokens.texts_to_sequences(train[:,0])\n",
    "X_train = keras.utils.pad_sequences(X_train_seq, 15, padding='post')\n",
    "Y_train_seq = l2_tokens.texts_to_sequences(train[:,1])\n",
    "Y_train = keras.utils.pad_sequences(Y_train_seq, 15, padding='post')\n",
    "\n",
    "X_test_seq = l1_tokens.texts_to_sequences(test[:,0])\n",
    "X_test = keras.utils.pad_sequences(X_test_seq, 15, padding='post')\n",
    "Y_test_seq = l2_tokens.texts_to_sequences(test[:,1])\n",
    "Y_test = keras.utils.pad_sequences(Y_test_seq, 15, padding='post')\n",
    "\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"Y_train.shape\", Y_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "print(\"Y_test.shape\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text data --> je ne pense pas que tom écoute\n",
      "numbers sequence --> [1, 6, 58, 3, 4, 11, 1747]\n",
      "padded sequence ---> [   1    6   58    3    4   11 1747    0    0    0    0    0    0    0\n",
      "    0]\n"
     ]
    }
   ],
   "source": [
    "print(\"text data -->\", train[15, 1])\n",
    "print(\"numbers sequence -->\", Y_train_seq[15])\n",
    "print(\"padded sequence --->\", Y_train[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 15, 256)           3755776   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               197120    \n",
      "                                                                 \n",
      " repeat_vector (RepeatVector  (None, 15, 128)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 15, 128)           131584    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 15, 33321)         4298409   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,382,889\n",
      "Trainable params: 8,382,889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(l1_vocab_size, 256, input_length = 15, mask_zero=True))\n",
    "model.add(LSTM(128))\n",
    "model.add(RepeatVector(15))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dense(l2_vocab_size, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "155/155 [==============================] - 16s 69ms/step - loss: 4.7406\n",
      "Epoch 2/30\n",
      "155/155 [==============================] - 11s 69ms/step - loss: 3.4694\n",
      "Epoch 3/30\n",
      "155/155 [==============================] - 11s 69ms/step - loss: 3.2092\n",
      "Epoch 4/30\n",
      "155/155 [==============================] - 11s 69ms/step - loss: 3.0279\n",
      "Epoch 5/30\n",
      "155/155 [==============================] - 11s 69ms/step - loss: 2.9618\n",
      "Epoch 6/30\n",
      "155/155 [==============================] - 11s 69ms/step - loss: 2.9328\n",
      "Epoch 7/30\n",
      "155/155 [==============================] - 11s 69ms/step - loss: 2.9096\n",
      "Epoch 8/30\n",
      "155/155 [==============================] - 11s 69ms/step - loss: 2.8888\n",
      "Epoch 9/30\n",
      "155/155 [==============================] - 11s 69ms/step - loss: 2.8676\n",
      "Epoch 10/30\n",
      "155/155 [==============================] - 11s 69ms/step - loss: 2.8139\n",
      "Epoch 11/30\n",
      "155/155 [==============================] - 11s 69ms/step - loss: 2.7545\n",
      "Epoch 12/30\n",
      "155/155 [==============================] - 11s 68ms/step - loss: 2.7057\n",
      "Epoch 13/30\n",
      "155/155 [==============================] - 11s 69ms/step - loss: 2.6617\n",
      "Epoch 14/30\n",
      "155/155 [==============================] - 11s 69ms/step - loss: 2.6018\n",
      "Epoch 15/30\n",
      "155/155 [==============================] - 11s 69ms/step - loss: 2.5384\n",
      "Epoch 16/30\n",
      "155/155 [==============================] - 11s 69ms/step - loss: 2.4844\n",
      "Epoch 17/30\n",
      "155/155 [==============================] - 11s 69ms/step - loss: 2.4316\n",
      "Epoch 18/30\n",
      "155/155 [==============================] - 11s 69ms/step - loss: 2.3774\n",
      "Epoch 19/30\n",
      "155/155 [==============================] - 11s 70ms/step - loss: 2.3202\n",
      "Epoch 20/30\n",
      "155/155 [==============================] - 11s 70ms/step - loss: 2.2656\n",
      "Epoch 21/30\n",
      "155/155 [==============================] - 11s 70ms/step - loss: 2.2114\n",
      "Epoch 22/30\n",
      "155/155 [==============================] - 11s 69ms/step - loss: 2.1591\n",
      "Epoch 23/30\n",
      "155/155 [==============================] - 11s 69ms/step - loss: 2.1076\n",
      "Epoch 24/30\n",
      "155/155 [==============================] - 11s 69ms/step - loss: 2.0551\n",
      "Epoch 25/30\n",
      "155/155 [==============================] - 11s 69ms/step - loss: 2.0029\n",
      "Epoch 26/30\n",
      "155/155 [==============================] - 11s 69ms/step - loss: 1.9523\n",
      "Epoch 27/30\n",
      "155/155 [==============================] - 11s 69ms/step - loss: 1.9052\n",
      "Epoch 28/30\n",
      "155/155 [==============================] - 11s 70ms/step - loss: 1.8599\n",
      "Epoch 29/30\n",
      "155/155 [==============================] - 11s 69ms/step - loss: 1.8177\n",
      "Epoch 30/30\n",
      "155/155 [==============================] - 11s 69ms/step - loss: 1.7758\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "history = model.fit(X_train, Y_train.reshape(Y_train.shape[0], Y_train.shape[1], 1), \n",
    "                    epochs=30, verbose=1, batch_size=1024)\n",
    "model.save_weights('eng_fra_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_line_prediction(text1, m):\n",
    "    #Given below is the code for pre-processing.  \n",
    "    def to_lines(text):\n",
    "        sents = text.strip().split('\\n')\n",
    "        sents = [i.split('\\t') for i in sents]\n",
    "        return sents\n",
    "    \n",
    "    small_input = to_lines(text1)\n",
    "    small_input = np.array(small_input)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    small_input[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in small_input[:,0]]\n",
    "    # convert text to lowercase\n",
    "    for i in range(len(small_input)):\n",
    "        small_input[i,0] = small_input[i,0].lower()\n",
    "\n",
    "    #encode and pad sequences\n",
    "    small_input_seq=l1_tokens.texts_to_sequences(small_input[0])\n",
    "    small_input= keras.utils.pad_sequences(small_input_seq,15,padding='post')\n",
    "   \n",
    "\n",
    "    #Using the code below, we load the model and get the prediction sequence. \n",
    "    #model.load_weights('/content/drive/My Drive/Training/Book/0.Chapters/Chapter12 RNN and LSTM/1.Archives/Eng_fra_model_v2.hdf5')\n",
    "\n",
    "    pred_seq = m.predict(small_input[0:1].reshape((small_input[0:1].shape[0],small_input[0:1].shape[1])), verbose=0)\n",
    "    print(pred_seq.shape)\n",
    "    #print(pred_seq)\n",
    "    \n",
    "    pred1 = [np.argmax(i) for i in pred_seq[0]]\n",
    "    print(pred1)\n",
    "    \n",
    "    def num_to_word(n, tokens):\n",
    "        for word, index in tokens.word_index.items():\n",
    "            if index == n:\n",
    "                return word\n",
    "        return None\n",
    "\n",
    "    Lang2_text = []\n",
    "    for wid in pred1:\n",
    "        t = num_to_word(wid, l2_tokens)\n",
    "        if t != None:\n",
    "            Lang2_text.append(t)\n",
    "\n",
    "    return(' '.join(Lang2_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 15, 33321)\n",
      "[9, 9, 36, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'vous vous avec'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_line_prediction(\"are you ok baby\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 15, 256)           3755776   \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 128)               197120    \n",
      "                                                                 \n",
      " repeat_vector_1 (RepeatVect  (None, 15, 128)          0         \n",
      " or)                                                             \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 15, 128)           131584    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 15, 33321)         4298409   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,382,889\n",
      "Trainable params: 8,382,889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m2 = Sequential()\n",
    "m2.add(Embedding(l1_vocab_size, 256, input_length = 15, mask_zero=True))\n",
    "m2.add(LSTM(128))\n",
    "m2.add(RepeatVector(15))\n",
    "m2.add(LSTM(128, return_sequences=True))\n",
    "m2.add(Dense(l2_vocab_size, activation='softmax'))\n",
    "m2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2.load_weights(r'/home/sudarsun/projects/ML_DL_py_TF/Chapter12_RNN_LSTM_V3/Datasets/Pre_trained_models/Eng_fra_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 15, 33321)\n",
      "[78, 4, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'estce que tu'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_line_prediction(\"are you ok baby\", m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
