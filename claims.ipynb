{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import functools\n",
    "from sklearn import tree, model_selection\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "import gc\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree, export_text\n",
    "import sys\n",
    "from sklearn.tree import _tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class colors:\n",
    "    '''Colors class:reset all colors with colors.reset; two\n",
    "    sub classes fg for foreground\n",
    "    and bg for background; use as colors.subclass.colorname.\n",
    "    i.e. colors.fg.red or colors.bg.greenalso, the generic bold, disable,\n",
    "    underline, reverse, strike through,\n",
    "    and invisible work with the main class i.e. colors.bold'''\n",
    "    reset = '\\033[0m'\n",
    "    bold = '\\033[01m'\n",
    "    disable = '\\033[02m'\n",
    "    underline = '\\033[04m'\n",
    "    reverse = '\\033[07m'\n",
    "    strikethrough = '\\033[09m'\n",
    "    invisible = '\\033[08m'\n",
    "\n",
    "    class fg:\n",
    "        black = '\\033[30m'\n",
    "        red = '\\033[31m'\n",
    "        green = '\\033[32m'\n",
    "        orange = '\\033[33m'\n",
    "        blue = '\\033[34m'\n",
    "        purple = '\\033[35m'\n",
    "        cyan = '\\033[36m'\n",
    "        lightgrey = '\\033[37m'\n",
    "        darkgrey = '\\033[90m'\n",
    "        lightred = '\\033[91m'\n",
    "        lightgreen = '\\033[92m'\n",
    "        yellow = '\\033[93m'\n",
    "        lightblue = '\\033[94m'\n",
    "        pink = '\\033[95m'\n",
    "        lightcyan = '\\033[96m'\n",
    "\n",
    "    class bg:\n",
    "        black = '\\033[40m'\n",
    "        red = '\\033[41m'\n",
    "        green = '\\033[42m'\n",
    "        orange = '\\033[43m'\n",
    "        blue = '\\033[44m'\n",
    "        purple = '\\033[45m'\n",
    "        cyan = '\\033[46m'\n",
    "        lightgrey = '\\033[47m'\n",
    "\n",
    "#print(colors.bg.green, \"SKk\", colors.fg.red, \"Amartya\")\n",
    "#print(colors.bg.lightgrey, \"SKk\", colors.fg.red, \"Amartya\")\n",
    "\n",
    "sys.setrecursionlimit(5000)\n",
    "def get_rules(tree, feature_names, class_names):\n",
    "    tree_ = tree.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "\n",
    "    paths = []\n",
    "    path = []\n",
    "    \n",
    "    def recurse(node, path, paths):\n",
    "        \n",
    "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            p1, p2 = list(path), list(path)\n",
    "            p1 += [f\"({name} <= {np.round(threshold, 3)})\"]\n",
    "            recurse(tree_.children_left[node], p1, paths)\n",
    "            p2 += [f\"({name} > {np.round(threshold, 3)})\"]\n",
    "            recurse(tree_.children_right[node], p2, paths)\n",
    "        else:\n",
    "            path += [(tree_.value[node], tree_.n_node_samples[node])]\n",
    "            paths += [path]\n",
    "            \n",
    "    recurse(0, path, paths)\n",
    "\n",
    "    # sort by samples count\n",
    "    samples_count = [p[-1][1] for p in paths]\n",
    "    ii = list(np.argsort(samples_count))\n",
    "    paths = [paths[i] for i in reversed(ii)]\n",
    "    \n",
    "    rules = []\n",
    "    for path in paths:\n",
    "        rule = \"if \"\n",
    "        \n",
    "        for p in path[:-1]:\n",
    "            if rule != \"if \":\n",
    "                rule += \" and \"\n",
    "            rule += str(p)\n",
    "        rule += \" then \"\n",
    "        if class_names is None:\n",
    "            rule += \"response: \"+str(np.round(path[-1][0][0][0],3))\n",
    "        else:\n",
    "            classes = path[-1][0][0]\n",
    "            l = np.argmax(classes)\n",
    "            rule += f\"class: {class_names[l]} (proba: {np.round(100.0*classes[l]/np.sum(classes),2)}%)\"\n",
    "        rule += f\" | based on {path[-1][1]:,} samples\"\n",
    "        rules += [rule]\n",
    "        \n",
    "    return rules\n",
    "\n",
    "def uniform_prior(prior):\n",
    "    result = prior.copy()\n",
    "    spread = 1./len(prior)\n",
    "    for k in prior.keys():\n",
    "        result[k] = spread\n",
    "    return result\n",
    "\n",
    "# function to fix the prior prob of PASS class, the rest of the classes shall have uniform prob.\n",
    "def fix_PASS_prior(prior, value):\n",
    "    result = prior.copy()\n",
    "    remain = (1 - value)/(len(prior)-1)\n",
    "    for k in prior.keys():\n",
    "        if k == \"PASS\":\n",
    "            result[k] = value\n",
    "        else:\n",
    "            result[k] = remain\n",
    "    return result\n",
    "\n",
    "def encode_column(coldata, index):\n",
    "    encoded = []\n",
    "    for r in coldata:\n",
    "        # check for nan by doing this comparison.\n",
    "        if r != r:\n",
    "            encoded.append([])\n",
    "        else:\n",
    "            # first filter the invalid codes from the list.\n",
    "            valid_codes = list(filter(lambda x: x in index, str(r).split(\",\")))\n",
    "            # map the codes to the index values.\n",
    "            codes = list(map(lambda x: index[x], valid_codes)) #str(r).split(\",\")))\n",
    "            encoded.append(codes)\n",
    "    return encoded\n",
    "\n",
    "def transform_denial_code(c, denial_index):\n",
    "    if c != c:\n",
    "        return 0\n",
    "    return denial_index[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(r'/home/sudarsun/notebooks/Sudar_sir_claims_scrubber_data_historical_v1.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['claim_id', 'patient_id', 'payer_id', 'plan_name', 'denial_code',\n",
       "       'code_activity', 'activity_desc', 'type_activity', 'act_type_dsc',\n",
       "       'pdx', 'sdx', 'Reason_for_visit', 'consolidated_diagnoses'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrecords = len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASS         478574\n",
      "MNEC-004     102590\n",
      "PRCE-002       7419\n",
      "PRCE-001       7305\n",
      "CODE-010       4061\n",
      "MNEC-005       3894\n",
      "CLAI-012       1827\n",
      "NCOV-0026      1518\n",
      "DUPL-002       1413\n",
      "PRCE-006        609\n",
      "COPY-001        400\n",
      "NCOV-001        337\n",
      "AUTH-001        308\n",
      "PRCE-010        286\n",
      "CODE-014        245\n",
      "ELIG-006        216\n",
      "NCOV-003        213\n",
      "ELIG-001        210\n",
      "PRCE-007        186\n",
      "TIME-001        178\n",
      "CLAI-008        117\n",
      "ELIG-007        116\n",
      "AUTH-003        105\n",
      "CLAI-016         62\n",
      "BENX-002         59\n",
      "BENX-005         57\n",
      "AUTH-005         50\n",
      "ELIG-005         43\n",
      "MNEC-003         42\n",
      "AUTH-004         29\n",
      "DUPL-001          2\n",
      "CLAI-018          1\n",
      "Name: denial_code, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# handle NAs in the denial code outputs\n",
    "data[\"denial_code\"] = data[\"denial_code\"].fillna(\"PASS\")\n",
    "dc_distrib = data[\"denial_code\"].value_counts()\n",
    "print(dc_distrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels found ['PASS' 'MNEC-004' 'PRCE-010' 'PRCE-002' 'CLAI-012' 'MNEC-005' 'CODE-010'\n",
      " 'DUPL-002' 'PRCE-001' 'PRCE-006' 'CODE-014' 'AUTH-001' 'AUTH-003'\n",
      " 'CLAI-016' 'PRCE-007' 'NCOV-003' 'NCOV-001' 'AUTH-005' 'ELIG-005'\n",
      " 'ELIG-001' 'ELIG-007' 'ELIG-006' 'CLAI-008' 'MNEC-003' 'AUTH-004'\n",
      " 'NCOV-0026' 'TIME-001' 'BENX-002' 'COPY-001' 'BENX-005' 'CLAI-018'\n",
      " 'DUPL-001']\n"
     ]
    }
   ],
   "source": [
    "denial_codes = pd.unique(data[\"denial_code\"])\n",
    "print(\"labels found\", denial_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the required denial codes of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class labels of interest ['PASS', 'MNEC-004']\n"
     ]
    }
   ],
   "source": [
    "# set the classes of interest.\n",
    "#denial_codes = list(dc_distrib.keys())\n",
    "denial_codes = [\"PASS\", \"MNEC-004\"]\n",
    "print('class labels of interest', denial_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter the cpt codes by some constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now filter the CPT codes that are starting with \"8\"\n",
    "selected_cpts = []\n",
    "for code in data[\"code_activity\"]:\n",
    "    if str(code)[0] == '8':\n",
    "        selected_cpts.append(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique CPT codes -> [84481 86376 84443 84439 80061 84100 82248 80053 85025 83525 82533 82565\n",
      " 82310 80051 82947 82024 82040 84450 83036 84460 84520 85610 85730 83202\n",
      " 81001 87510 87660 87480 82607 82728 84630 84550 86431 86140 82306 83540\n",
      " 83735 83970 82785 86003 86703 86900 86901 86803 87340 85060 84436 82247\n",
      " 83690 84295 84132 80076 82465 84702 84478 84484 82043 85652 84153 85379\n",
      " 84466 82746 82330 83520 82308 86800 84432 89322 87804 82378 86301 80047\n",
      " 82150 82272 83993 87169 87338 87425 82962 84075 88142 82670 83001 83002\n",
      " 84146 87086 83550 82550 83021 86592 86762 80069 82105 84305 84403 84156\n",
      " 86225 86038 82977 86160 82274 87230 82805 87807 83615 87880 82374 83090\n",
      " 86147 85300 85303 86039 86060 86200 86850 82627 82960 82955 86304 84144\n",
      " 84270 84402 83498 86885 86778 86777 86644 86645 83516 87809 87899 87040\n",
      " 87045 86235 84154 86000 87220 87270 87081 87210 87205 87481 84480 86300\n",
      " 87070 87076 86141 83013 83014 82340 88720 87177 87329 82784 85045 84155\n",
      " 82553 85660 82435 87801 87075 84681 82951 82157 82950 86706 86707 86709\n",
      " 87350 86704 85384 86787 86255 86256 86340 82941 84445 86708 86880 82570\n",
      " 84163 84704 87158 86077 80074 85018 86886 83872 80651 84101 80631 83101\n",
      " 87071 83880 87186 85732 84145 82104 82390 87641 87635 87106 87073 83721\n",
      " 83718 86148 86705 88305 82626 82945 86663 86664 86665 85032 82747 86671\n",
      " 87209 86628 86738 88173 85613 85670 85705 83835 82088 86812 85306 86622\n",
      " 86341 83519 85044 86480 82803 86694 82656 87624 84220 82672 84425 82668\n",
      " 84165 86695 86696 82085 86337 87184 87206 87077 86308 84560 82575 84446\n",
      " 82270 82164 82542 86146 82652 85049 86593 83201 80299 86336 82480 82180\n",
      " 84590 86316 81400 85240 85245 88112 87118 86317 85210 85230 85250 85270\n",
      " 85280 84540 86927 83003 88304 86677 88184 82595 84206 82103 82787 87592\n",
      " 83883 81002 88185 88187 84133 84300 84376 86780 86335 80177 86320 86334\n",
      " 84166 84244 82677 85247 86747 87116 87147 84060 86816 86817 82365 82383\n",
      " 88307 82615 84392 85307 84207 82525 87517 86606 82608 81270 82375 81219\n",
      " 87046 83527 87185 82610 86332 85246 87516 87799 88230 88262 88291 81257\n",
      " 87107 82530 80197 89050 89060 83935 83930 84442 82239 87999 82232 86580\n",
      " 84252 81291 83010 88342 87623 87172 84260 83528 84157 87530 80178 84301\n",
      " 86753 87522 88312 86021 83497 84588 82108 85220 83020 83937 86360 84201\n",
      " 87491 87591 84105 85301 83921 84203 88188 88313 85540 80175 82261 86611\n",
      " 83605 81335 85611 82757 86615 82658 87912 82985 84585 82380 86352 84597\n",
      " 83986 87521 87556 82160 89051 83102 82360 81208 83625 85260 87102 87557\n",
      " 81361 81256 86790 83719 83009 82671 82373 83945 87625 82436 85244 81223\n",
      " 81224 81378 81232 85536 87207 85290 81253 85014 84134 81269 87101 86813\n",
      " 86735 84255 84106 84110 86682 88346 82135 80202 81507 87274 87273 87497\n",
      " 81259 86022 87337 86355 86359 82382 82507 81025 81207 88348 80157 88360\n",
      " 86325 87278 88321 82355 86674 86698 83874 84080 86618 82131 80156 87084\n",
      " 86023 82384]\n",
      "# unique CPTs:  470\n"
     ]
    }
   ],
   "source": [
    "# we are strongly assuming that the data is not multi-label.\n",
    "# get the unique cpt codes\n",
    "#cpt = pd.unique(list(data[\"code_activity\"]))\n",
    "cpt = pd.unique(selected_cpts)\n",
    "print(\"unique CPT codes ->\", cpt)\n",
    "print(\"# unique CPTs: \", len(cpt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear previously owned\n",
    "if \"pdx_icds\" in locals():\n",
    "    # release the memory blocks\n",
    "    del(pdx_icds)\n",
    "    del(sdx_icds)\n",
    "    del(rov_icds)\n",
    "    gc.collect()\n",
    "    \n",
    "# collect the icd codes from the diagnosis columns\n",
    "pdx_icds = [str(code).split(',') for code in data[data[\"pdx\"].notna()][\"pdx\"]]\n",
    "sdx_icds = [str(code).split(',') for code in data[data[\"sdx\"].notna()][\"sdx\"]] \n",
    "rov_icds = [str(code).split(',') for code in data[data[\"Reason_for_visit\"].notna()][\"Reason_for_visit\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the pdx items\n",
    "pdx = []\n",
    "for row in pdx_icds:\n",
    "    for code in row:\n",
    "        pdx.append(code)\n",
    "# collect the sdx items\n",
    "sdx = []\n",
    "for row in sdx_icds:\n",
    "    for code in row:\n",
    "        sdx.append(code)\n",
    "# collect the rov items        \n",
    "rov = []\n",
    "for row in rov_icds:\n",
    "    for code in row:\n",
    "        rov.append(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set the ICD min support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique icd codes -> ['E55.9' 'R53.83' 'D50.9' ... 'H01.131' 'T23.212A' 'S56.424D']\n",
      "# unique icds:  5337\n"
     ]
    }
   ],
   "source": [
    "# identify the codes that does not have the min-support\n",
    "icd_minsupport = 5\n",
    "\n",
    "# compute the ICD distribution\n",
    "icd_distrib = pd.DataFrame(pdx + sdx + rov).value_counts()\n",
    "\n",
    "# now filter the ICD codes that don't have the required support.\n",
    "icd_to_retain = np.array(list(filter(lambda x: icd_distrib[x] >= icd_minsupport, icd_distrib.keys()))).flatten()\n",
    "\n",
    "print(\"unique icd codes ->\", icd_to_retain)\n",
    "print(\"# unique icds: \", len(icd_to_retain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward and reverse index the unique codes.\n",
    "icd_indices = dict((i,c) for c,i in enumerate(icd_to_retain))\n",
    "cpt_indices = dict((str(c),i) for i, c in enumerate(cpt))\n",
    "denial_indices = dict((str(c),i) for i, c in enumerate(denial_codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PASS': 0, 'MNEC-004': 1}\n",
      "['PASS', 'MNEC-004']\n"
     ]
    }
   ],
   "source": [
    "print(denial_indices)\n",
    "print(denial_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the type activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_activity = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Also filter by CPT codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/store/userdata/sudarsun/claims/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  after removing the cwd from sys.path.\n",
      "/mnt/store/userdata/sudarsun/claims/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \"\"\"\n",
      "/mnt/store/userdata/sudarsun/claims/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \n",
      "/mnt/store/userdata/sudarsun/claims/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASS        292166\n",
      "MNEC-004     92458\n",
      "Name: denial_code, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# construct a new data frame with only the necessary data.\n",
    "dc_array = denial_codes.copy()\n",
    "# also limit the scope of only CPT codes, type_activity=3\n",
    "new_data = data[data[\"denial_code\"] == dc_array[0]][data[\"type_activity\"]==type_activity]\n",
    "new_data = new_data[new_data[\"code_activity\"] < 90000][new_data[\"code_activity\"] >= 80000]\n",
    "dc_array.pop(0)\n",
    "for dc in dc_array:\n",
    "    mydf = data[data[\"denial_code\"] == dc][data[\"type_activity\"]==type_activity]\n",
    "    mydf = mydf[mydf[\"code_activity\"] < 90000][mydf[\"code_activity\"] >= 80000]\n",
    "    new_data = new_data.append(mydf)\n",
    "print(new_data[\"denial_code\"].value_counts())\n",
    "\n",
    "# update the no of records.\n",
    "nrecords = len(new_data)\n",
    "\n",
    "# @TODO we should destroy the original data frame for memory saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASS        0.759615\n",
      "MNEC-004    0.240385\n",
      "Name: denial_code, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# encode the claim result\n",
    "#claim_status = new_data[\"denial_code\"] == \"PASS\"\n",
    "#let's first compute the class prior\n",
    "#prior = claim_status.value_counts(normalize=True)\n",
    "\n",
    "prior = new_data[\"denial_code\"].value_counts(dropna=False, normalize=True)\n",
    "print(prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the training and testing priors as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Required test prior:\n",
      " PASS        0.759615\n",
      "MNEC-004    0.240385\n",
      "Name: denial_code, dtype: float64\n",
      "\n",
      "Required train prior:\n",
      " PASS        0.7\n",
      "MNEC-004    0.3\n",
      "Name: denial_code, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# set the required testing data prior\n",
    "test_prior = prior\n",
    "#test_prior = uniform_prior(prior)\n",
    "print(\"\\nRequired test prior:\\n\",test_prior)\n",
    "\n",
    "# override the train prior if needed\n",
    "#train_prior = prior\n",
    "#train_prior = uniform_prior(prior)\n",
    "train_prior = fix_PASS_prior(prior, 0.7)\n",
    "print(\"\\nRequired train prior:\\n\", train_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set the required training and testing corpus sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the number of required training & testing sample sizes\n",
    "ntrain = 307000\n",
    "ntest = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASS        292166\n",
      "MNEC-004     92458\n",
      "Name: denial_code, dtype: int64\n",
      "\u001b[34m class: PASS available= 292166 req_train= 214900 req_test= 759\n",
      "\u001b[34m class: MNEC-004 available= 92458 req_train= 92100 req_test= 240\n"
     ]
    }
   ],
   "source": [
    "available = new_data[\"denial_code\"].value_counts()\n",
    "print(available)\n",
    "for dc in denial_codes:\n",
    "    av = available[dc]\n",
    "    tr = int(train_prior[dc]*ntrain)\n",
    "    te = int(test_prior[dc]*ntest)\n",
    "    print(colors.fg.blue, \"class:\", dc, \"available=\", av, \"req_train=\", tr, \"req_test=\", te)\n",
    "    if (tr+te) > av:\n",
    "        print(colors.bold, colors.fg.red, \"\\nPROBLEM: class\", dc, \"required\", tr+te, \"> available\",av,\"please adjust ntrain and ntest\", colors.reset)\n",
    "        sys.exit(1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASS --> available points: 292166 || required train points: 214900 || test points: 759\n",
      "got 214900 train & 759 test points.\n",
      "MNEC-004 --> available points: 92458 || required train points: 92100 || test points: 240\n",
      "got 92100 train & 240 test points.\n",
      "\n",
      "Required train data prior:\n",
      " PASS        0.7\n",
      "MNEC-004    0.3\n",
      "Name: denial_code, dtype: float64\n",
      "\n",
      "Actual train data prior:\n",
      " PASS        0.7\n",
      "MNEC-004    0.3\n",
      "Name: denial_code, dtype: float64\n",
      "\n",
      "Required test data prior:\n",
      " PASS        0.759615\n",
      "MNEC-004    0.240385\n",
      "Name: denial_code, dtype: float64\n",
      "\n",
      "Actual test data prior:\n",
      " PASS        0.75976\n",
      "MNEC-004    0.24024\n",
      "Name: denial_code, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# now collect the ids for training and testing samples.\n",
    "train_ids = []    \n",
    "test_ids = []\n",
    "\n",
    "cum_train = 0\n",
    "cum_test = 0\n",
    "# now, we have to draw the sample from tr_indexed data points, hopefully following the class prior.\n",
    "for c in prior.keys():\n",
    "    # get the required training data count\n",
    "    req_ntrain = int(train_prior[c] * ntrain)\n",
    "    req_ntest = int(test_prior[c] * ntest)\n",
    "    \n",
    "    # get the keys from the data\n",
    "    #keys = data[data[\"denial_code\"]==c][\"denial_code\"].keys()\n",
    "    keys = new_data[new_data[\"denial_code\"]==c][\"denial_code\"].keys()\n",
    "    print(c, \"--> available points:\", len(keys), \"|| required train points:\", req_ntrain, \"|| test points:\", req_ntest)\n",
    "\n",
    "    # permutate the keys.\n",
    "    keys = np.random.permutation(keys)\n",
    "    \n",
    "    # if the required count is less than available, slice the array into train and test.\n",
    "    if len(keys) > req_ntrain + req_ntest:\n",
    "        train_ids = train_ids + list(keys[0:req_ntrain])\n",
    "        test_ids = test_ids + list(keys[req_ntrain:(req_ntrain+req_ntest)])\n",
    "    else:\n",
    "        print(colors.bold, colors.fg.red, \"\\nPROBLEM: required samples greater than available, please adjust ntrain and ntest\", colors.reset)\n",
    "        break\n",
    "    \n",
    "    #for k in keys:\n",
    "    #    if k in tr_index and len(train_ids)-cum_train < req_ntrain:\n",
    "    #        train_ids.append(k)\n",
    "    #    if k in te_index and len(test_ids)-cum_test < req_ntest:\n",
    "    #        test_ids.append(k)\n",
    "    print(\"got\", len(train_ids)-cum_train, \"train &\", len(test_ids)-cum_test, \"test points.\")\n",
    "    cum_train = len(train_ids)\n",
    "    cum_test = len(test_ids)\n",
    "\n",
    "print(\"\\nRequired train data prior:\\n\", train_prior)\n",
    "print(\"\\nActual train data prior:\\n\", new_data[\"denial_code\"][train_ids].value_counts(normalize=True))\n",
    "print(\"\\nRequired test data prior:\\n\", test_prior)\n",
    "print(\"\\nActual test data prior:\\n\", new_data[\"denial_code\"][test_ids].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pop=PASS~214900,MNEC-004~92100~~tr=PASS~0.7,MNEC-004~0.3~~val=PASS~0.76,MNEC-004~0.24~~~cpt=8xxxx\n"
     ]
    }
   ],
   "source": [
    "# create the model name based on the settings.\n",
    "distrib = new_data[\"denial_code\"][train_ids].value_counts()\n",
    "part1 = \",\".join(map(lambda x: x + \"~\" + str(distrib[x]), distrib.keys()))\n",
    "tr_distrib = new_data[\"denial_code\"][train_ids].value_counts(normalize=True)\n",
    "part2 = \",\".join(map(lambda x: x + \"~\" + str(round(tr_distrib[x],2)), tr_distrib.keys()))\n",
    "te_distrib = new_data[\"denial_code\"][test_ids].value_counts(normalize=True)\n",
    "part3 = \",\".join(map(lambda x: x + \"~\" + str(round(te_distrib[x],2)), te_distrib.keys()))\n",
    "model_name = \"pop=\" + part1 + \"~~tr=\" + part2 + \"~~val=\" + part3\n",
    "\n",
    "model_name += \"~~cpt=8XXXX\"\n",
    "\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode the PDX column\n",
    "pdx_encoded_tr = encode_column(new_data[\"pdx\"][train_ids], icd_indices)\n",
    "pdx_encoded_te = encode_column(new_data[\"pdx\"][test_ids], icd_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode the SDX column\n",
    "sdx_encoded_tr = encode_column(new_data[\"sdx\"][train_ids], icd_indices)\n",
    "sdx_encoded_te = encode_column(new_data[\"sdx\"][test_ids], icd_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the ROV column\n",
    "rov_encoded_tr = encode_column(new_data[\"Reason_for_visit\"][train_ids], icd_indices)\n",
    "rov_encoded_te = encode_column(new_data[\"Reason_for_visit\"][test_ids], icd_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the CPT column\n",
    "cpt_encoded_tr = encode_column(new_data[\"code_activity\"][train_ids], cpt_indices)\n",
    "cpt_encoded_te = encode_column(new_data[\"code_activity\"][test_ids], cpt_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the denial column\n",
    "denial_encoded_tr = list(map(lambda x: transform_denial_code(x, denial_indices), new_data[\"denial_code\"][train_ids]))\n",
    "denial_encoded_te = list(map(lambda x: transform_denial_code(x, denial_indices), new_data[\"denial_code\"][test_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(record, num_classes):\n",
    "    encoded = np.zeros(num_classes)\n",
    "    for r in record:\n",
    "        encoded[r] = 1.\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free up the space explicitly\n",
    "if 'X_train' in locals():\n",
    "    del(X_train) \n",
    "    del(X_test)\n",
    "    gc.collect()\n",
    "\n",
    "X_train = [np.hstack((one_hot(cpt_encoded_tr[i], num_classes=len(cpt_indices)), \n",
    "                      one_hot(rov_encoded_tr[i], num_classes=len(icd_indices)),\n",
    "                      one_hot(sdx_encoded_tr[i], num_classes=len(icd_indices)),\n",
    "                      one_hot(pdx_encoded_tr[i], num_classes=len(icd_indices)))) for i in range(len(train_ids))]\n",
    "X_test  = [np.hstack((one_hot(cpt_encoded_te[i], num_classes=len(cpt_indices)), \n",
    "                      one_hot(rov_encoded_te[i], num_classes=len(icd_indices)),\n",
    "                      one_hot(sdx_encoded_te[i], num_classes=len(icd_indices)),\n",
    "                      one_hot(pdx_encoded_te[i], num_classes=len(icd_indices)))) for i in range(len(test_ids))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16481\n"
     ]
    }
   ],
   "source": [
    "# free up the space explicitly\n",
    "if 'features' in locals():\n",
    "    del(features)\n",
    "    gc.collect()\n",
    "\n",
    "features = list(cpt_indices.keys()) + [\"RFV-\"+k for k in icd_indices.keys()] + [\"SDX-\"+k for k in icd_indices.keys()] + [\"PDX-\"+k for k in icd_indices.keys()]\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name += \"~~n_features=\" + str(len(features))\n",
    "model_name += \"~~type-activity=\" + str(type_activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m MODEL file pop=PASS~214900,MNEC-004~92100~~tr=PASS~0.7,MNEC-004~0.3~~val=PASS~0.76,MNEC-004~0.24~~~cpt=8xxxx~~n_features=16481~~type-activity=3 is not available, building it.. \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model_already_available = True\n",
    "#check if the file is already available, if so, load it.\n",
    "try:\n",
    "    with open(model_name + \".pkl\", \"rb\") as model_fp:\n",
    "        print(colors.fg.blue, \"MODEL file\", model_name + \".pkl\", \"is already available, loading it..\" ,colors.reset)\n",
    "        dt = pickle.load(model_fp)\n",
    "        model_fp.close()\n",
    "except FileNotFoundError:\n",
    "    print(colors.fg.red, \"MODEL file\", model_name + \".pkl\", \"is not available, building it..\" ,colors.reset)\n",
    "    # if the file is not present, let's create the model freshly.\n",
    "    model_already_available = False\n",
    "    dt = tree.DecisionTreeClassifier(min_samples_leaf=50, class_weight=\"balanced\")\n",
    "    dt.fit(X_train, denial_encoded_tr)\n",
    "    # save the model.\n",
    "    with open(model_name + \".pkl\", \"wb\") as file:\n",
    "        pickle.dump(dt, file)\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's run the prediction only if the model is built from scratch\n",
    "if model_already_available == False:\n",
    "    y_pred = dt.predict(X_test)\n",
    "    y_hat_train = dt.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.81      0.87    214900\n",
      "           1       0.67      0.88      0.76     92100\n",
      "\n",
      "    accuracy                           0.83    307000\n",
      "   macro avg       0.80      0.85      0.82    307000\n",
      "weighted avg       0.86      0.83      0.84    307000\n",
      "\n",
      "Accuracy = 0.8334820846905537\n"
     ]
    }
   ],
   "source": [
    "# let's run the prediction only if the model is built from scratch\n",
    "if model_already_available == False:\n",
    "    print(classification_report(denial_encoded_tr, y_hat_train, zero_division=0))\n",
    "    print(\"Accuracy =\", accuracy_score(denial_encoded_tr, y_hat_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.82      0.88       759\n",
      "           1       0.61      0.88      0.72       240\n",
      "\n",
      "    accuracy                           0.84       999\n",
      "   macro avg       0.78      0.85      0.80       999\n",
      "weighted avg       0.87      0.84      0.85       999\n",
      "\n",
      "Test Accuracy = 0.8368368368368369\n"
     ]
    }
   ],
   "source": [
    "# let's run the prediction only if the model is built from scratch\n",
    "if model_already_available == False:\n",
    "    #cm = confusion_matrix(y_test, y_pred)\n",
    "    cm = confusion_matrix(denial_encoded_te, y_pred)\n",
    "    #print(cm)\n",
    "\n",
    "    print(classification_report(denial_encoded_te, y_pred, zero_division=0))\n",
    "    print(\"Test Accuracy =\", accuracy_score(denial_encoded_te, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model.\n",
    "#with open(model_name, \"wb\") as file:\n",
    "#    pickle.dump(dt, file)\n",
    "#    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf = RandomForestClassifier(n_jobs=10)\n",
    "#rf.fit(X_train, denial_encoded_tr)\n",
    "#rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yhat = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(100,40))\n",
    "#plot_tree(dt, filled=True, rounded=True, impurity=True, feature_names=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the rules now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extract the induced rules and save them to a file.\n",
    "rules = get_rules(dt, features, denial_codes)\n",
    "rule_text = \"\\n\".join(rules)\n",
    "\n",
    "with open(model_name + \".rules\", \"wt\") as rule_file:\n",
    "    rule_file.write(rule_text)\n",
    "    rule_file.close()\n",
    "    \n",
    "    #for r in rules:\n",
    "        #print(r)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
