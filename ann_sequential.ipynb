{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the data (5351, 3)\n",
      "random sample:\n",
      "          w1      w2       w3\n",
      "4553   love      it     when\n",
      "269    hate      it     when\n",
      "1996   love     you      for\n",
      "2862   love      to      see\n",
      "419   hated      it     when\n",
      "4909  loved     her  husband\n",
      "3462   love      to      see\n",
      "1545   love     her      and\n",
      "687    love  affair     with\n",
      "2875   love      to      see\n"
     ]
    }
   ],
   "source": [
    "column_names = ['w1', 'w2', 'w3']\n",
    "trigrams = pd.read_csv(r'/home/sudarsun/projects/ML_DL_py_TF/Chapter12_RNN_LSTM_V3/Datasets/3Gram_love_data.txt', \n",
    "                       delimiter='\\t',\n",
    "                       names=column_names)\n",
    "print('shape of the data', trigrams.shape)\n",
    "print('random sample:\\n', trigrams.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Frequency of w1 values\n",
      " love      4327\n",
      "loved      416\n",
      "hate       400\n",
      "hated       80\n",
      "loves       72\n",
      "lovely      24\n",
      "loving      24\n",
      "hates        8\n",
      "Name: w1, dtype: int64\n",
      "\n",
      "Frequency of w2 values\n",
      " to         1866\n",
      "it         1361\n",
      "the         548\n",
      "with        240\n",
      "him         144\n",
      "you         144\n",
      "of          136\n",
      "her         104\n",
      "for          96\n",
      "and          88\n",
      "what         56\n",
      "is           48\n",
      "each         40\n",
      "in           40\n",
      "ones         32\n",
      "me           32\n",
      "nothing      32\n",
      "them         32\n",
      "as           24\n",
      "every        24\n",
      "more         16\n",
      "going        16\n",
      "that         16\n",
      "being        16\n",
      "affair       16\n",
      "my           16\n",
      "about         8\n",
      "your          8\n",
      "on            8\n",
      "letter        8\n",
      "most          8\n",
      "thy           8\n",
      "view          8\n",
      "song          8\n",
      "makes         8\n",
      "got           8\n",
      "this          8\n",
      "at            8\n",
      "a             8\n",
      "one           8\n",
      "hearing       8\n",
      "story         8\n",
      "all           8\n",
      "lost          8\n",
      "man           8\n",
      "when          8\n",
      "husband       8\n",
      "Name: w2, dtype: int64\n",
      "\n",
      "Frequency of w3 values\n",
      " when       1305\n",
      "see        1098\n",
      "way         380\n",
      "the         168\n",
      "so          120\n",
      "           ... \n",
      "respect       8\n",
      "nature        8\n",
      "kind          8\n",
      "take          8\n",
      "being         8\n",
      "Name: w3, Length: 111, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFrequency of w1 values\\n\", trigrams[\"w1\"].value_counts())\n",
    "print(\"\\nFrequency of w2 values\\n\", trigrams[\"w2\"].value_counts())\n",
    "print(\"\\nFrequency of w3 values\\n\", trigrams[\"w3\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of unique words: 139\n",
      "unique word list: ['a' 'able' 'about' 'admit' 'affair' 'affection' 'all' 'and' 'another'\n",
      " 'answer' 'as' 'at' 'be' 'because' 'being' 'better' 'between' 'bother'\n",
      " 'break' 'care' 'cared' 'come' 'concern' 'country' 'cut' 'disappoint' 'do'\n",
      " 'each' 'every' 'fact' 'feel' 'feeling' 'find' 'first' 'for' 'from' 'get'\n",
      " 'go' 'god' 'going' 'got' 'hate' 'hated' 'hates' 'have' 'he' 'hear'\n",
      " 'hearing' 'her' 'here' 'him' 'his' 'husband' 'i' 'idea' 'if' 'in'\n",
      " 'interrupt' 'is' 'it' 'kind' 'know' 'leave' 'letter' 'life' 'like'\n",
      " 'listen' 'look' 'lost' 'lot' 'love' 'loved' 'lovely' 'loves' 'loving'\n",
      " 'make' 'makes' 'man' 'marriage' 'me' 'minute' 'more' 'most' 'much'\n",
      " 'music' 'my' 'nature' 'neighbor' 'not' 'nothing' 'of' 'on' 'one' 'ones'\n",
      " 'or' 'other' 'over' 'play' 'respect' 'say' 'see' 'sit' 'smell' 'so'\n",
      " 'someone' 'song' 'sound' 'story' 'stronger' 'support' 'take' 'talk'\n",
      " 'tell' 'than' 'that' 'the' 'them' 'they' 'think' 'this' 'thought' 'thy'\n",
      " 'to' 'too' 'united' 'use' 'very' 'view' 'watch' 'way' 'we' 'what' 'when'\n",
      " 'wife' 'will' 'with' 'work' 'you' 'your']\n"
     ]
    }
   ],
   "source": [
    "unique_words = []\n",
    "for i in list(trigrams.columns.values):\n",
    "    for j in pd.unique(trigrams[i]):\n",
    "        unique_words.append(j)\n",
    "unique_words = np.unique(unique_words)\n",
    "\n",
    "print('count of unique words:', len(unique_words))\n",
    "print('unique word list:', unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_indices dictionary\n",
      " {'a': 0, 'able': 1, 'about': 2, 'admit': 3, 'affair': 4, 'affection': 5, 'all': 6, 'and': 7, 'another': 8, 'answer': 9, 'as': 10, 'at': 11, 'be': 12, 'because': 13, 'being': 14, 'better': 15, 'between': 16, 'bother': 17, 'break': 18, 'care': 19, 'cared': 20, 'come': 21, 'concern': 22, 'country': 23, 'cut': 24, 'disappoint': 25, 'do': 26, 'each': 27, 'every': 28, 'fact': 29, 'feel': 30, 'feeling': 31, 'find': 32, 'first': 33, 'for': 34, 'from': 35, 'get': 36, 'go': 37, 'god': 38, 'going': 39, 'got': 40, 'hate': 41, 'hated': 42, 'hates': 43, 'have': 44, 'he': 45, 'hear': 46, 'hearing': 47, 'her': 48, 'here': 49, 'him': 50, 'his': 51, 'husband': 52, 'i': 53, 'idea': 54, 'if': 55, 'in': 56, 'interrupt': 57, 'is': 58, 'it': 59, 'kind': 60, 'know': 61, 'leave': 62, 'letter': 63, 'life': 64, 'like': 65, 'listen': 66, 'look': 67, 'lost': 68, 'lot': 69, 'love': 70, 'loved': 71, 'lovely': 72, 'loves': 73, 'loving': 74, 'make': 75, 'makes': 76, 'man': 77, 'marriage': 78, 'me': 79, 'minute': 80, 'more': 81, 'most': 82, 'much': 83, 'music': 84, 'my': 85, 'nature': 86, 'neighbor': 87, 'not': 88, 'nothing': 89, 'of': 90, 'on': 91, 'one': 92, 'ones': 93, 'or': 94, 'other': 95, 'over': 96, 'play': 97, 'respect': 98, 'say': 99, 'see': 100, 'sit': 101, 'smell': 102, 'so': 103, 'someone': 104, 'song': 105, 'sound': 106, 'story': 107, 'stronger': 108, 'support': 109, 'take': 110, 'talk': 111, 'tell': 112, 'than': 113, 'that': 114, 'the': 115, 'them': 116, 'they': 117, 'think': 118, 'this': 119, 'thought': 120, 'thy': 121, 'to': 122, 'too': 123, 'united': 124, 'use': 125, 'very': 126, 'view': 127, 'watch': 128, 'way': 129, 'we': 130, 'what': 131, 'when': 132, 'wife': 133, 'will': 134, 'with': 135, 'work': 136, 'you': 137, 'your': 138}\n",
      "word_indices.keys\n",
      " dict_keys(['a', 'able', 'about', 'admit', 'affair', 'affection', 'all', 'and', 'another', 'answer', 'as', 'at', 'be', 'because', 'being', 'better', 'between', 'bother', 'break', 'care', 'cared', 'come', 'concern', 'country', 'cut', 'disappoint', 'do', 'each', 'every', 'fact', 'feel', 'feeling', 'find', 'first', 'for', 'from', 'get', 'go', 'god', 'going', 'got', 'hate', 'hated', 'hates', 'have', 'he', 'hear', 'hearing', 'her', 'here', 'him', 'his', 'husband', 'i', 'idea', 'if', 'in', 'interrupt', 'is', 'it', 'kind', 'know', 'leave', 'letter', 'life', 'like', 'listen', 'look', 'lost', 'lot', 'love', 'loved', 'lovely', 'loves', 'loving', 'make', 'makes', 'man', 'marriage', 'me', 'minute', 'more', 'most', 'much', 'music', 'my', 'nature', 'neighbor', 'not', 'nothing', 'of', 'on', 'one', 'ones', 'or', 'other', 'over', 'play', 'respect', 'say', 'see', 'sit', 'smell', 'so', 'someone', 'song', 'sound', 'story', 'stronger', 'support', 'take', 'talk', 'tell', 'than', 'that', 'the', 'them', 'they', 'think', 'this', 'thought', 'thy', 'to', 'too', 'united', 'use', 'very', 'view', 'watch', 'way', 'we', 'what', 'when', 'wife', 'will', 'with', 'work', 'you', 'your'])\n",
      "word_indices.values\n",
      " dict_values([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138])\n",
      "\n",
      "##################################################\n",
      "indices_words dictionary\n",
      " {0: 'a', 1: 'able', 2: 'about', 3: 'admit', 4: 'affair', 5: 'affection', 6: 'all', 7: 'and', 8: 'another', 9: 'answer', 10: 'as', 11: 'at', 12: 'be', 13: 'because', 14: 'being', 15: 'better', 16: 'between', 17: 'bother', 18: 'break', 19: 'care', 20: 'cared', 21: 'come', 22: 'concern', 23: 'country', 24: 'cut', 25: 'disappoint', 26: 'do', 27: 'each', 28: 'every', 29: 'fact', 30: 'feel', 31: 'feeling', 32: 'find', 33: 'first', 34: 'for', 35: 'from', 36: 'get', 37: 'go', 38: 'god', 39: 'going', 40: 'got', 41: 'hate', 42: 'hated', 43: 'hates', 44: 'have', 45: 'he', 46: 'hear', 47: 'hearing', 48: 'her', 49: 'here', 50: 'him', 51: 'his', 52: 'husband', 53: 'i', 54: 'idea', 55: 'if', 56: 'in', 57: 'interrupt', 58: 'is', 59: 'it', 60: 'kind', 61: 'know', 62: 'leave', 63: 'letter', 64: 'life', 65: 'like', 66: 'listen', 67: 'look', 68: 'lost', 69: 'lot', 70: 'love', 71: 'loved', 72: 'lovely', 73: 'loves', 74: 'loving', 75: 'make', 76: 'makes', 77: 'man', 78: 'marriage', 79: 'me', 80: 'minute', 81: 'more', 82: 'most', 83: 'much', 84: 'music', 85: 'my', 86: 'nature', 87: 'neighbor', 88: 'not', 89: 'nothing', 90: 'of', 91: 'on', 92: 'one', 93: 'ones', 94: 'or', 95: 'other', 96: 'over', 97: 'play', 98: 'respect', 99: 'say', 100: 'see', 101: 'sit', 102: 'smell', 103: 'so', 104: 'someone', 105: 'song', 106: 'sound', 107: 'story', 108: 'stronger', 109: 'support', 110: 'take', 111: 'talk', 112: 'tell', 113: 'than', 114: 'that', 115: 'the', 116: 'them', 117: 'they', 118: 'think', 119: 'this', 120: 'thought', 121: 'thy', 122: 'to', 123: 'too', 124: 'united', 125: 'use', 126: 'very', 127: 'view', 128: 'watch', 129: 'way', 130: 'we', 131: 'what', 132: 'when', 133: 'wife', 134: 'will', 135: 'with', 136: 'work', 137: 'you', 138: 'your'}\n",
      "indices_words.keys\n",
      " dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138])\n",
      "indices_words.values\n",
      " dict_values(['a', 'able', 'about', 'admit', 'affair', 'affection', 'all', 'and', 'another', 'answer', 'as', 'at', 'be', 'because', 'being', 'better', 'between', 'bother', 'break', 'care', 'cared', 'come', 'concern', 'country', 'cut', 'disappoint', 'do', 'each', 'every', 'fact', 'feel', 'feeling', 'find', 'first', 'for', 'from', 'get', 'go', 'god', 'going', 'got', 'hate', 'hated', 'hates', 'have', 'he', 'hear', 'hearing', 'her', 'here', 'him', 'his', 'husband', 'i', 'idea', 'if', 'in', 'interrupt', 'is', 'it', 'kind', 'know', 'leave', 'letter', 'life', 'like', 'listen', 'look', 'lost', 'lot', 'love', 'loved', 'lovely', 'loves', 'loving', 'make', 'makes', 'man', 'marriage', 'me', 'minute', 'more', 'most', 'much', 'music', 'my', 'nature', 'neighbor', 'not', 'nothing', 'of', 'on', 'one', 'ones', 'or', 'other', 'over', 'play', 'respect', 'say', 'see', 'sit', 'smell', 'so', 'someone', 'song', 'sound', 'story', 'stronger', 'support', 'take', 'talk', 'tell', 'than', 'that', 'the', 'them', 'they', 'think', 'this', 'thought', 'thy', 'to', 'too', 'united', 'use', 'very', 'view', 'watch', 'way', 'we', 'what', 'when', 'wife', 'will', 'with', 'work', 'you', 'your'])\n"
     ]
    }
   ],
   "source": [
    "word_indices = dict((w, i) for i, w in enumerate(unique_words))\n",
    "indices_words = dict((i, w) for i, w in enumerate(unique_words))\n",
    "\n",
    "print(\"word_indices dictionary\\n\", word_indices)\n",
    "print(\"word_indices.keys\\n\", word_indices.keys())\n",
    "print(\"word_indices.values\\n\", word_indices.values())\n",
    "print(\"\\n\" + \"#\"*50)\n",
    "print(\"indices_words dictionary\\n\", indices_words)\n",
    "print(\"indices_words.keys\\n\", indices_words.keys())\n",
    "print(\"indices_words.values\\n\", indices_words.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word1_onehot shape is (5351, 139)\n"
     ]
    }
   ],
   "source": [
    "### one hot encoding\n",
    "w1 = trigrams['w1'].map(word_indices)\n",
    "w1_hot = keras.utils.to_categorical(np.array(w1), num_classes=len(word_indices))\n",
    "print(\"word1_onehot shape is\", w1_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The word in row 500 is --- >love\n",
      "The one-hot encoded value is\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nThe word in row 500 is --- >\" + trigrams['w1'][500])\n",
    "print(\"The one-hot encoded value is\\n\", w1_hot[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2_onehot shape is (5351, 139)\n"
     ]
    }
   ],
   "source": [
    "w2 = trigrams['w2'].map(word_indices)\n",
    "w2_hot = keras.utils.to_categorical(np.array(w2), num_classes=len(word_indices))\n",
    "print(\"word2_onehot shape is\", w2_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word3_onehot shape is (5351, 139)\n"
     ]
    }
   ],
   "source": [
    "w3 = trigrams['w3'].map(word_indices)\n",
    "w3_hot = keras.utils.to_categorical(np.array(w3), num_classes=len(word_indices))\n",
    "print(\"word3_onehot shape is\", w3_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 10)                1400      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 139)               1529      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,929\n",
      "Trainable params: 2,929\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Dense(10, input_dim = w1_hot.shape[1], activation='sigmoid'))\n",
    "model1.add(Dense(w2_hot.shape[1], activation='softmax'))\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "536/536 [==============================] - 2s 3ms/step - loss: 0.0212 - accuracy: 0.3487\n",
      "Epoch 2/20\n",
      "536/536 [==============================] - 2s 3ms/step - loss: 0.0211 - accuracy: 0.3511\n",
      "Epoch 3/20\n",
      "536/536 [==============================] - 2s 3ms/step - loss: 0.0210 - accuracy: 0.3526\n",
      "Epoch 4/20\n",
      "536/536 [==============================] - 2s 3ms/step - loss: 0.0210 - accuracy: 0.3532\n",
      "Epoch 5/20\n",
      "536/536 [==============================] - 2s 3ms/step - loss: 0.0210 - accuracy: 0.3532\n",
      "Epoch 6/20\n",
      "536/536 [==============================] - 2s 3ms/step - loss: 0.0210 - accuracy: 0.3532\n",
      "Epoch 7/20\n",
      "536/536 [==============================] - 2s 3ms/step - loss: 0.0210 - accuracy: 0.3532\n",
      "Epoch 8/20\n",
      "536/536 [==============================] - 2s 3ms/step - loss: 0.0210 - accuracy: 0.3532\n",
      "Epoch 9/20\n",
      "536/536 [==============================] - 2s 3ms/step - loss: 0.0210 - accuracy: 0.3526\n",
      "Epoch 10/20\n",
      "536/536 [==============================] - 2s 3ms/step - loss: 0.0210 - accuracy: 0.3526\n",
      "Epoch 11/20\n",
      "536/536 [==============================] - 2s 3ms/step - loss: 0.0210 - accuracy: 0.3532\n",
      "Epoch 12/20\n",
      "536/536 [==============================] - 2s 3ms/step - loss: 0.0210 - accuracy: 0.3526\n",
      "Epoch 13/20\n",
      "536/536 [==============================] - 2s 3ms/step - loss: 0.0210 - accuracy: 0.3532\n",
      "Epoch 14/20\n",
      "536/536 [==============================] - 2s 3ms/step - loss: 0.0210 - accuracy: 0.3532\n",
      "Epoch 15/20\n",
      "536/536 [==============================] - 2s 3ms/step - loss: 0.0210 - accuracy: 0.3532\n",
      "Epoch 16/20\n",
      "536/536 [==============================] - 2s 3ms/step - loss: 0.0210 - accuracy: 0.3532\n",
      "Epoch 17/20\n",
      "536/536 [==============================] - 2s 3ms/step - loss: 0.0210 - accuracy: 0.3532\n",
      "Epoch 18/20\n",
      "536/536 [==============================] - 2s 3ms/step - loss: 0.0210 - accuracy: 0.3532\n",
      "Epoch 19/20\n",
      "536/536 [==============================] - 2s 3ms/step - loss: 0.0210 - accuracy: 0.3532\n",
      "Epoch 20/20\n",
      "536/536 [==============================] - 2s 3ms/step - loss: 0.0210 - accuracy: 0.3528\n"
     ]
    }
   ],
   "source": [
    "model1.compile(loss='binary_crossentropy', \n",
    "               optimizer='adam',#keras.optimizers.Adam(learning_rate=0.1, beta_1=.9, beta_2=.999), \n",
    "               metrics=['accuracy'])\n",
    "history = model1.fit(w1_hot, w2_hot, epochs=20, batch_size=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168/168 [==============================] - 0s 896us/step\n",
      "the hidden layer output for every record - shape of it\n",
      " (5351, 10)\n",
      "few records from hidden layer\n",
      " [[2.3638018e-08 1.2690427e-04 2.0301753e-01 4.9049643e-08 6.1551300e-06\n",
      "  4.4475041e-09 4.2696669e-05 1.2246055e-07 3.0104186e-08 5.3861093e-10]\n",
      " [2.3638018e-08 1.2690427e-04 2.0301753e-01 4.9049643e-08 6.1551300e-06\n",
      "  4.4475041e-09 4.2696669e-05 1.2246055e-07 3.0104186e-08 5.3861093e-10]\n",
      " [2.3638018e-08 1.2690427e-04 2.0301753e-01 4.9049643e-08 6.1551300e-06\n",
      "  4.4475041e-09 4.2696669e-05 1.2246055e-07 3.0104186e-08 5.3861093e-10]\n",
      " [2.3638018e-08 1.2690427e-04 2.0301753e-01 4.9049643e-08 6.1551300e-06\n",
      "  4.4475041e-09 4.2696669e-05 1.2246055e-07 3.0104186e-08 5.3861093e-10]\n",
      " [2.3638018e-08 1.2690427e-04 2.0301753e-01 4.9049643e-08 6.1551300e-06\n",
      "  4.4475041e-09 4.2696669e-05 1.2246055e-07 3.0104186e-08 5.3861093e-10]]\n"
     ]
    }
   ],
   "source": [
    "model1_hidden = Sequential()\n",
    "model1_hidden.add(Dense(10, input_dim=w1_hot.shape[1], weights=model1.layers[0].get_weights()))\n",
    "model1_hidden.add(Activation(\"sigmoid\"))\n",
    "\n",
    "# getting the hidden layer activation\n",
    "model1_hidden_output = model1_hidden.predict(w1_hot)\n",
    "# peak into our hidden layer activations\n",
    "print(\"the hidden layer output for every record - shape of it\\n\", model1_hidden_output.shape)\n",
    "print(\"few records from hidden layer\\n\", model1_hidden_output[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2 hidden append shape (5351, 149)\n"
     ]
    }
   ],
   "source": [
    "w2_hidden_append = np.append(model1_hidden_output, w2_hot, axis=1)\n",
    "print(\"word2 hidden append shape\", w2_hidden_append.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_9 (Dense)             (None, 10)                1500      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 139)               1529      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,029\n",
      "Trainable params: 3,029\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(10, input_dim=w2_hidden_append.shape[1], activation='sigmoid'))\n",
    "model2.add(Dense(w3_hot.shape[1],  activation='softmax'))\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "536/536 [==============================] - 2s 3ms/step - loss: 0.2574 - accuracy: 0.1353\n",
      "Epoch 2/20\n",
      "536/536 [==============================] - 2s 3ms/step - loss: 0.0535 - accuracy: 0.2439\n",
      "Epoch 3/20\n",
      "536/536 [==============================] - 2s 3ms/step - loss: 0.0365 - accuracy: 0.2439\n",
      "Epoch 4/20\n",
      "536/536 [==============================] - 2s 3ms/step - loss: 0.0326 - accuracy: 0.2439\n",
      "Epoch 5/20\n",
      "536/536 [==============================] - 2s 3ms/step - loss: 0.0312 - accuracy: 0.2439\n",
      "Epoch 6/20\n",
      "536/536 [==============================] - 2s 3ms/step - loss: 0.0306 - accuracy: 0.2439\n",
      "Epoch 7/20\n",
      "536/536 [==============================] - 2s 3ms/step - loss: 0.0303 - accuracy: 0.2439\n",
      "Epoch 8/20\n",
      "536/536 [==============================] - 2s 3ms/step - loss: 0.0301 - accuracy: 0.2439\n",
      "Epoch 9/20\n",
      "536/536 [==============================] - 2s 3ms/step - loss: 0.0299 - accuracy: 0.2439\n",
      "Epoch 10/20\n",
      "536/536 [==============================] - 2s 3ms/step - loss: 0.0296 - accuracy: 0.2519\n",
      "Epoch 11/20\n",
      "536/536 [==============================] - 2s 3ms/step - loss: 0.0292 - accuracy: 0.3392\n",
      "Epoch 12/20\n",
      "536/536 [==============================] - 2s 3ms/step - loss: 0.0284 - accuracy: 0.4491\n",
      "Epoch 13/20\n",
      "536/536 [==============================] - 2s 3ms/step - loss: 0.0272 - accuracy: 0.4491\n",
      "Epoch 14/20\n",
      "536/536 [==============================] - 2s 3ms/step - loss: 0.0259 - accuracy: 0.4653\n",
      "Epoch 15/20\n",
      "536/536 [==============================] - 2s 3ms/step - loss: 0.0247 - accuracy: 0.5177\n",
      "Epoch 16/20\n",
      "536/536 [==============================] - 2s 3ms/step - loss: 0.0237 - accuracy: 0.5201\n",
      "Epoch 17/20\n",
      "536/536 [==============================] - 2s 3ms/step - loss: 0.0229 - accuracy: 0.5201\n",
      "Epoch 18/20\n",
      "536/536 [==============================] - 2s 3ms/step - loss: 0.0221 - accuracy: 0.5332\n",
      "Epoch 19/20\n",
      "536/536 [==============================] - 2s 3ms/step - loss: 0.0215 - accuracy: 0.5448\n",
      "Epoch 20/20\n",
      "536/536 [==============================] - 2s 3ms/step - loss: 0.0209 - accuracy: 0.5485\n"
     ]
    }
   ],
   "source": [
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history2 = model2.fit(w2_hidden_append, w3_hot, epochs=20, batch_size=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_step_pred(words_in):\n",
    "    index_input = word_indices[words_in[0]]\n",
    "    indices_in = keras.utils.to_categorical(index_input, num_classes=len(word_indices))\n",
    "    indices_in = indices_in.reshape(1, len(word_indices))\n",
    "    h1_test = model1_hidden(indices_in)\n",
    "    \n",
    "    index_input2 = word_indices[words_in[1]]\n",
    "    indices_in2 = keras.utils.to_categorical(index_input2, num_classes=len(word_indices))\n",
    "    indices_in2 = indices_in2.reshape(1, len(word_indices))\n",
    "    x2_test = np.append(h1_test, indices_in2, axis=1)\n",
    "    \n",
    "    yhat = np.argmax(model2.predict(x2_test), axis=1)\n",
    "    \n",
    "    print(\"input words -->\", words_in)\n",
    "    print(\"predicted words -->\", indices_words[yhat[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n",
      "input words --> ['hate', 'the']\n",
      "predicted words --> way\n"
     ]
    }
   ],
   "source": [
    "two_step_pred(['hate', 'the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
